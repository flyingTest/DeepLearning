{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前排定义一下训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# 训练循环次数\n",
    "num_epochs = 50\n",
    "\n",
    "# batch大小\n",
    "batch_size = 256\n",
    "\n",
    "# lstm层中包含的unit个数\n",
    "rnn_size = 256\n",
    "\n",
    "# lstm层数\n",
    "num_layers = 3\n",
    "\n",
    "# 训练步长\n",
    "seq_length = 30\n",
    "\n",
    "# 学习率\n",
    "learning_rate = 0.001\n",
    "\n",
    "#dropout keep\n",
    "output_keep_prob = 0.8\n",
    "input_keep_prob = 1.0\n",
    "\n",
    "# 优化器\n",
    "grad_clip = 5.\n",
    "\n",
    "decay_rate = 0.97\n",
    "init_from = None\n",
    "save_every = 1000\n",
    "# 保存模型\n",
    "save_dir = './save'\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    assert False, \"你为创建保存模型文件，已为你创建 文件夹名：save\"\n",
    "# 保存logs   \n",
    "log_dir = './logs'\n",
    "if not os.path.isdir(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    assert False, \"你为创建logs文件，已为你创建 文件夹名：logs\"\n",
    "# 保存数据和词汇\n",
    "data_dir = './temp'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    assert False, \"你为创建数据储存文件，已为你创建 文件夹名：temp\"\n",
    "    \n",
    "input_file = os.path.join(data_dir, \"爵迹I II.txt\")\n",
    "if not os.path.exists(input_file): \n",
    "    print('请将郭小四的小说放到temp文件夹下....')  \n",
    "vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "_file = os.path.join(save_dir, 'chars_vocab.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先加载数据集 <p>\n",
    "使用到的是**`爵迹`**这本小说<p>\n",
    "无论小说和电影都能给人很深刻的印象...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, 'r',encoding = 'gbk') as f:\n",
    "        text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预览一下部分内容<p>**\n",
    "果然一股东方神话、字里行间透露出45度角仰望天空的忧伤气息扑面而来<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'而来？传说中至高无上的【白银祭司】又掌握着怎样的真相？这场旷世之战，究竟要将主角的命运引向王者的宝座，还是惨烈的死亡？\\n\\n    \\n\\n    序章  神遇\\n\\n    \\n\\n    漫天翻滚的碎雪，仿佛巨兽抖落的白色绒毛，纷纷扬扬地遮蔽着视线。\\n\\n    这块大陆的冬天已经来临。\\n\\n    南方只是开始不易察觉地降温，凌晨的时候窗棂上会看见霜花，但是在这里——大陆接近极北的尽头，已经是一望无际的苍茫肃杀。大块大块浮动在海面上的冰山彼此不时地撞击着，在天地间发出巨大的锐利轰鸣声，坍塌的冰块砸进大海，掀起白色的浪涛。辽阔的黑色冻土在接连几天的大雪之后，变成了一片茫茫的雪原。这已经是深北之地了，连绵不断'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[500:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 做一些数据预处理，去掉一写无关的字符和空格，去掉书籍前几行没用的介绍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile('\\[.*\\]|<.*>|\\.+|【|】| +|\\\\r|\\\\n')\n",
    "text = pattern.sub('', text.strip()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'巨兽抖落的白色绒毛，纷纷扬扬地遮蔽着视线。这块大陆的冬天已经来临。南方只是开始不易察觉地降温，凌晨的时候窗棂上会看见霜花，但是在这里——大陆接近极北的尽头，已经是一望无际的苍茫肃杀。大块大块浮动在海面上的冰山彼此不时地撞击着，在天地间发出巨大的锐利轰鸣声，坍塌的冰块砸进大海，掀起白色的浪涛。辽阔的黑色冻土在接连几天的大雪之后，变成了一片茫茫的雪原。这已经是深北之地了，连绵不断的冰川仿佛怪兽的利齿般将天地的尽头紧紧咬在一起，地平线消失在刺眼的白色冰面之下。天空被厚重的云层遮挡，光线仿佛蒙着一层尘埃，混沌地洒向大地。混沌的风雪在空旷的天地间吹出一阵又一阵仿佛狼嗥般的凄厉声响。拳头大小的纷乱大雪里，'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[500:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  感觉预处理后效果还可以.没那么乱了，开始做词映射\n",
    "1. 首先做词频统计，再降序排序，因为用的是char级的所以这一步是没什么必要的，统计有多少个汉字和字符，其实可以用``chars=set(text)``代替\n",
    "2. 将统计结果作为语料库，存入本地pkl文件中，方便调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from six.moves import cPickle\n",
    "counter = collections.Counter(text)\n",
    "counter = sorted(counter.items(), key=lambda x: -x[1])\n",
    "chars, _  = zip(*counter)\n",
    "with open(vocab_file, 'wb') as f:\n",
    "    cPickle.dump(chars, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对词汇表字符(包括\\n哦)做一个数字索引，并用这个数字索引来代替这个汉字<p>\n",
    "保存字词映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(chars)\n",
    "vocab = dict(zip(chars, range(vocab_size)))\n",
    "with open(_file, 'wb') as f:\n",
    "    cPickle.dump((chars, vocab), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 将整本书的内容，做一下 汉字/字符  - 数字 的变化。\n",
    "2. 这样原来的一本书变可以用一个由N个数字组成的列表表示了\n",
    "3. 最后把向量化的这本书保存下来，方便之后调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "text_tensor = np.array(list(map(vocab.get, text)))\n",
    "np.save(tensor_file, text_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建训练所需数据格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = int(text_tensor.size / (batch_size * seq_length))\n",
    "\n",
    "if num_batches == 0:\n",
    "    assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "text_tensor = text_tensor[: num_batches * batch_size * seq_length]\n",
    "xdata = text_tensor\n",
    "ydata = np.copy(text_tensor)\n",
    "\n",
    "#循环神经网络，最后一个输出为最先一个输入\n",
    "ydata[:-1] = xdata[1:]\n",
    "ydata[-1] = xdata[0]\n",
    "x_batches = np.split(xdata.reshape( batch_size, -1),\n",
    "                          num_batches, 1)\n",
    "y_batches = np.split(ydata.reshape(batch_size, -1),\n",
    "                          num_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个生成器,生成批次数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(pointer):\n",
    "    x, y = x_batches[pointer], y_batches[pointer]\n",
    "    return x, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建LSTM的cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not training:\n",
    "    batch_size = 1\n",
    "    seq_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = []\n",
    "for _ in range(num_layers):\n",
    "    cell = rnn.LSTMCell(rnn_size)\n",
    "    if training and (output_keep_prob < 1.0 or input_keep_prob < 1.0):\n",
    "        cell = rnn.DropoutWrapper(cell,\n",
    "                                  input_keep_prob=input_keep_prob,\n",
    "                                  output_keep_prob=output_keep_prob)\n",
    "    cells.append(cell)\n",
    "cell = rnn.MultiRNNCell(cells, state_is_tuple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化占位符,随机化参数矩阵，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length])\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "with tf.variable_scope('rnnlm'):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\",[rnn_size, vocab_size])\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将input转化为词嵌入向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\n",
    "inputs = tf.nn.embedding_lookup(embedding, input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout beta testing: double check which one should affect next line\n",
    "if training and output_keep_prob:\n",
    "    inputs = tf.nn.dropout(inputs, output_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拆散input_data放入rnn模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.split(inputs, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decoder的输出和最终状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, initial_state, cell,  scope='rnnlm')\n",
    "output = tf.reshape(tf.concat(outputs, 1), [-1, rnn_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对输出层做softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "probs = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "        [logits],\n",
    "        [tf.reshape(targets, [-1])],\n",
    "        [tf.ones([batch_size * seq_length])])\n",
    "with tf.name_scope('cost'):\n",
    "    cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "final_state = last_state\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),grad_clip)\n",
    "with tf.name_scope('optimizer'):\n",
    "    optimizer = tf.train.AdamOptimizer(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1900 (epoch 0), train_loss = 7.984, time/batch = 1.455\n",
      "model saved to ./save\\model.ckpt\n",
      "1/1900 (epoch 0), train_loss = 7.982, time/batch = 1.354\n",
      "2/1900 (epoch 0), train_loss = 7.975, time/batch = 1.209\n",
      "3/1900 (epoch 0), train_loss = 7.949, time/batch = 1.164\n",
      "4/1900 (epoch 0), train_loss = 7.835, time/batch = 1.171\n",
      "5/1900 (epoch 0), train_loss = 7.539, time/batch = 1.180\n",
      "6/1900 (epoch 0), train_loss = 7.251, time/batch = 1.163\n",
      "7/1900 (epoch 0), train_loss = 7.036, time/batch = 1.179\n",
      "8/1900 (epoch 0), train_loss = 6.815, time/batch = 1.173\n",
      "9/1900 (epoch 0), train_loss = 6.602, time/batch = 1.213\n",
      "10/1900 (epoch 0), train_loss = 6.440, time/batch = 1.647\n",
      "11/1900 (epoch 0), train_loss = 6.308, time/batch = 1.394\n",
      "12/1900 (epoch 0), train_loss = 6.246, time/batch = 1.418\n",
      "13/1900 (epoch 0), train_loss = 6.185, time/batch = 1.612\n",
      "14/1900 (epoch 0), train_loss = 6.175, time/batch = 1.733\n",
      "15/1900 (epoch 0), train_loss = 6.210, time/batch = 1.569\n",
      "16/1900 (epoch 0), train_loss = 6.205, time/batch = 1.440\n",
      "17/1900 (epoch 0), train_loss = 6.184, time/batch = 1.442\n",
      "18/1900 (epoch 0), train_loss = 6.203, time/batch = 1.477\n",
      "19/1900 (epoch 0), train_loss = 6.225, time/batch = 1.498\n",
      "20/1900 (epoch 0), train_loss = 6.238, time/batch = 1.202\n",
      "21/1900 (epoch 0), train_loss = 6.235, time/batch = 1.179\n",
      "22/1900 (epoch 0), train_loss = 6.165, time/batch = 1.255\n",
      "23/1900 (epoch 0), train_loss = 6.192, time/batch = 1.221\n",
      "24/1900 (epoch 0), train_loss = 6.226, time/batch = 1.292\n",
      "25/1900 (epoch 0), train_loss = 6.207, time/batch = 1.492\n",
      "26/1900 (epoch 0), train_loss = 6.167, time/batch = 1.593\n",
      "27/1900 (epoch 0), train_loss = 6.163, time/batch = 1.555\n",
      "28/1900 (epoch 0), train_loss = 6.139, time/batch = 1.645\n",
      "29/1900 (epoch 0), train_loss = 6.174, time/batch = 1.534\n",
      "30/1900 (epoch 0), train_loss = 6.103, time/batch = 1.354\n",
      "31/1900 (epoch 0), train_loss = 6.181, time/batch = 1.416\n",
      "32/1900 (epoch 0), train_loss = 6.171, time/batch = 1.412\n",
      "33/1900 (epoch 0), train_loss = 6.138, time/batch = 1.422\n",
      "34/1900 (epoch 0), train_loss = 6.133, time/batch = 1.458\n",
      "35/1900 (epoch 0), train_loss = 6.180, time/batch = 1.405\n",
      "36/1900 (epoch 0), train_loss = 6.156, time/batch = 1.369\n",
      "37/1900 (epoch 0), train_loss = 6.169, time/batch = 1.237\n",
      "38/1900 (epoch 1), train_loss = 6.316, time/batch = 1.518\n",
      "39/1900 (epoch 1), train_loss = 6.147, time/batch = 1.334\n",
      "40/1900 (epoch 1), train_loss = 6.142, time/batch = 1.214\n",
      "41/1900 (epoch 1), train_loss = 6.123, time/batch = 1.246\n",
      "42/1900 (epoch 1), train_loss = 6.151, time/batch = 1.235\n",
      "43/1900 (epoch 1), train_loss = 6.174, time/batch = 1.431\n",
      "44/1900 (epoch 1), train_loss = 6.143, time/batch = 1.614\n",
      "45/1900 (epoch 1), train_loss = 6.178, time/batch = 1.568\n",
      "46/1900 (epoch 1), train_loss = 6.147, time/batch = 1.511\n",
      "47/1900 (epoch 1), train_loss = 6.115, time/batch = 1.403\n",
      "48/1900 (epoch 1), train_loss = 6.115, time/batch = 1.497\n",
      "49/1900 (epoch 1), train_loss = 6.101, time/batch = 1.509\n",
      "50/1900 (epoch 1), train_loss = 6.125, time/batch = 1.430\n",
      "51/1900 (epoch 1), train_loss = 6.106, time/batch = 1.590\n",
      "52/1900 (epoch 1), train_loss = 6.117, time/batch = 1.534\n",
      "53/1900 (epoch 1), train_loss = 6.157, time/batch = 1.617\n",
      "54/1900 (epoch 1), train_loss = 6.156, time/batch = 1.579\n",
      "55/1900 (epoch 1), train_loss = 6.126, time/batch = 1.280\n",
      "56/1900 (epoch 1), train_loss = 6.127, time/batch = 1.319\n",
      "57/1900 (epoch 1), train_loss = 6.146, time/batch = 1.298\n",
      "58/1900 (epoch 1), train_loss = 6.150, time/batch = 1.332\n",
      "59/1900 (epoch 1), train_loss = 6.153, time/batch = 1.329\n",
      "60/1900 (epoch 1), train_loss = 6.101, time/batch = 1.307\n",
      "61/1900 (epoch 1), train_loss = 6.124, time/batch = 1.406\n",
      "62/1900 (epoch 1), train_loss = 6.172, time/batch = 1.426\n",
      "63/1900 (epoch 1), train_loss = 6.151, time/batch = 1.302\n",
      "64/1900 (epoch 1), train_loss = 6.132, time/batch = 1.401\n",
      "65/1900 (epoch 1), train_loss = 6.127, time/batch = 1.352\n",
      "66/1900 (epoch 1), train_loss = 6.117, time/batch = 1.399\n",
      "67/1900 (epoch 1), train_loss = 6.148, time/batch = 1.309\n",
      "68/1900 (epoch 1), train_loss = 6.077, time/batch = 1.288\n",
      "69/1900 (epoch 1), train_loss = 6.153, time/batch = 1.385\n",
      "70/1900 (epoch 1), train_loss = 6.150, time/batch = 1.469\n",
      "71/1900 (epoch 1), train_loss = 6.112, time/batch = 1.358\n",
      "72/1900 (epoch 1), train_loss = 6.108, time/batch = 1.378\n",
      "73/1900 (epoch 1), train_loss = 6.157, time/batch = 1.443\n",
      "74/1900 (epoch 1), train_loss = 6.135, time/batch = 1.309\n",
      "75/1900 (epoch 1), train_loss = 6.152, time/batch = 1.287\n",
      "76/1900 (epoch 2), train_loss = 6.260, time/batch = 1.373\n",
      "77/1900 (epoch 2), train_loss = 6.130, time/batch = 1.360\n",
      "78/1900 (epoch 2), train_loss = 6.130, time/batch = 1.249\n",
      "79/1900 (epoch 2), train_loss = 6.110, time/batch = 1.279\n",
      "80/1900 (epoch 2), train_loss = 6.147, time/batch = 1.331\n",
      "81/1900 (epoch 2), train_loss = 6.167, time/batch = 1.361\n",
      "82/1900 (epoch 2), train_loss = 6.132, time/batch = 1.389\n",
      "83/1900 (epoch 2), train_loss = 6.165, time/batch = 1.470\n",
      "84/1900 (epoch 2), train_loss = 6.140, time/batch = 1.174\n",
      "85/1900 (epoch 2), train_loss = 6.113, time/batch = 1.261\n",
      "86/1900 (epoch 2), train_loss = 6.103, time/batch = 1.377\n",
      "87/1900 (epoch 2), train_loss = 6.098, time/batch = 1.219\n",
      "88/1900 (epoch 2), train_loss = 6.126, time/batch = 1.541\n",
      "89/1900 (epoch 2), train_loss = 6.107, time/batch = 1.621\n",
      "90/1900 (epoch 2), train_loss = 6.115, time/batch = 1.462\n",
      "91/1900 (epoch 2), train_loss = 6.160, time/batch = 1.300\n",
      "92/1900 (epoch 2), train_loss = 6.151, time/batch = 1.296\n",
      "93/1900 (epoch 2), train_loss = 6.122, time/batch = 1.307\n",
      "94/1900 (epoch 2), train_loss = 6.126, time/batch = 1.209\n",
      "95/1900 (epoch 2), train_loss = 6.145, time/batch = 1.266\n",
      "96/1900 (epoch 2), train_loss = 6.148, time/batch = 1.544\n",
      "97/1900 (epoch 2), train_loss = 6.152, time/batch = 1.495\n",
      "98/1900 (epoch 2), train_loss = 6.101, time/batch = 1.486\n",
      "99/1900 (epoch 2), train_loss = 6.126, time/batch = 1.896\n",
      "100/1900 (epoch 2), train_loss = 6.167, time/batch = 1.611\n",
      "101/1900 (epoch 2), train_loss = 6.149, time/batch = 1.550\n",
      "102/1900 (epoch 2), train_loss = 6.135, time/batch = 1.491\n",
      "103/1900 (epoch 2), train_loss = 6.120, time/batch = 1.574\n",
      "104/1900 (epoch 2), train_loss = 6.114, time/batch = 1.922\n",
      "105/1900 (epoch 2), train_loss = 6.148, time/batch = 1.842\n",
      "106/1900 (epoch 2), train_loss = 6.075, time/batch = 1.657\n",
      "107/1900 (epoch 2), train_loss = 6.151, time/batch = 1.545\n",
      "108/1900 (epoch 2), train_loss = 6.152, time/batch = 1.611\n",
      "109/1900 (epoch 2), train_loss = 6.112, time/batch = 1.706\n",
      "110/1900 (epoch 2), train_loss = 6.107, time/batch = 1.448\n",
      "111/1900 (epoch 2), train_loss = 6.155, time/batch = 1.572\n",
      "112/1900 (epoch 2), train_loss = 6.135, time/batch = 1.473\n",
      "113/1900 (epoch 2), train_loss = 6.150, time/batch = 1.653\n",
      "114/1900 (epoch 3), train_loss = 6.239, time/batch = 1.420\n",
      "115/1900 (epoch 3), train_loss = 6.130, time/batch = 1.511\n",
      "116/1900 (epoch 3), train_loss = 6.124, time/batch = 1.536\n",
      "117/1900 (epoch 3), train_loss = 6.113, time/batch = 1.828\n",
      "118/1900 (epoch 3), train_loss = 6.138, time/batch = 1.792\n",
      "119/1900 (epoch 3), train_loss = 6.163, time/batch = 1.685\n",
      "120/1900 (epoch 3), train_loss = 6.129, time/batch = 1.440\n",
      "121/1900 (epoch 3), train_loss = 6.165, time/batch = 1.268\n",
      "122/1900 (epoch 3), train_loss = 6.139, time/batch = 1.347\n",
      "123/1900 (epoch 3), train_loss = 6.110, time/batch = 1.357\n",
      "124/1900 (epoch 3), train_loss = 6.108, time/batch = 1.227\n",
      "125/1900 (epoch 3), train_loss = 6.097, time/batch = 1.220\n",
      "126/1900 (epoch 3), train_loss = 6.123, time/batch = 1.217\n",
      "127/1900 (epoch 3), train_loss = 6.106, time/batch = 1.326\n",
      "128/1900 (epoch 3), train_loss = 6.114, time/batch = 1.553\n",
      "129/1900 (epoch 3), train_loss = 6.159, time/batch = 1.946\n",
      "130/1900 (epoch 3), train_loss = 6.154, time/batch = 1.544\n",
      "131/1900 (epoch 3), train_loss = 6.119, time/batch = 1.685\n",
      "132/1900 (epoch 3), train_loss = 6.126, time/batch = 1.825\n",
      "133/1900 (epoch 3), train_loss = 6.141, time/batch = 1.445\n",
      "134/1900 (epoch 3), train_loss = 6.146, time/batch = 1.383\n",
      "135/1900 (epoch 3), train_loss = 6.151, time/batch = 1.332\n",
      "136/1900 (epoch 3), train_loss = 6.096, time/batch = 1.331\n",
      "137/1900 (epoch 3), train_loss = 6.125, time/batch = 1.458\n",
      "138/1900 (epoch 3), train_loss = 6.165, time/batch = 1.612\n",
      "139/1900 (epoch 3), train_loss = 6.150, time/batch = 1.395\n",
      "140/1900 (epoch 3), train_loss = 6.131, time/batch = 1.433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/1900 (epoch 3), train_loss = 6.121, time/batch = 1.489\n",
      "142/1900 (epoch 3), train_loss = 6.110, time/batch = 1.219\n",
      "143/1900 (epoch 3), train_loss = 6.144, time/batch = 1.168\n",
      "144/1900 (epoch 3), train_loss = 6.070, time/batch = 1.153\n",
      "145/1900 (epoch 3), train_loss = 6.149, time/batch = 1.167\n",
      "146/1900 (epoch 3), train_loss = 6.148, time/batch = 1.156\n",
      "147/1900 (epoch 3), train_loss = 6.109, time/batch = 1.204\n",
      "148/1900 (epoch 3), train_loss = 6.106, time/batch = 1.296\n",
      "149/1900 (epoch 3), train_loss = 6.154, time/batch = 1.260\n",
      "150/1900 (epoch 3), train_loss = 6.137, time/batch = 1.336\n",
      "151/1900 (epoch 3), train_loss = 6.148, time/batch = 1.197\n",
      "152/1900 (epoch 4), train_loss = 6.215, time/batch = 1.197\n",
      "153/1900 (epoch 4), train_loss = 6.119, time/batch = 1.413\n",
      "154/1900 (epoch 4), train_loss = 6.132, time/batch = 1.774\n",
      "155/1900 (epoch 4), train_loss = 6.116, time/batch = 1.733\n",
      "156/1900 (epoch 4), train_loss = 6.146, time/batch = 1.533\n",
      "157/1900 (epoch 4), train_loss = 6.167, time/batch = 1.460\n",
      "158/1900 (epoch 4), train_loss = 6.139, time/batch = 1.520\n",
      "159/1900 (epoch 4), train_loss = 6.168, time/batch = 1.781\n",
      "160/1900 (epoch 4), train_loss = 6.144, time/batch = 1.628\n",
      "161/1900 (epoch 4), train_loss = 6.111, time/batch = 1.716\n",
      "162/1900 (epoch 4), train_loss = 6.114, time/batch = 1.440\n",
      "163/1900 (epoch 4), train_loss = 6.101, time/batch = 1.468\n",
      "164/1900 (epoch 4), train_loss = 6.129, time/batch = 1.691\n",
      "165/1900 (epoch 4), train_loss = 6.113, time/batch = 1.574\n",
      "166/1900 (epoch 4), train_loss = 6.117, time/batch = 1.668\n",
      "167/1900 (epoch 4), train_loss = 6.161, time/batch = 1.568\n",
      "168/1900 (epoch 4), train_loss = 6.154, time/batch = 1.572\n",
      "169/1900 (epoch 4), train_loss = 6.124, time/batch = 1.596\n",
      "170/1900 (epoch 4), train_loss = 6.130, time/batch = 1.436\n",
      "171/1900 (epoch 4), train_loss = 6.146, time/batch = 1.449\n",
      "172/1900 (epoch 4), train_loss = 6.155, time/batch = 1.680\n",
      "173/1900 (epoch 4), train_loss = 6.158, time/batch = 1.496\n",
      "174/1900 (epoch 4), train_loss = 6.110, time/batch = 1.551\n",
      "175/1900 (epoch 4), train_loss = 6.133, time/batch = 1.630\n",
      "176/1900 (epoch 4), train_loss = 6.173, time/batch = 1.590\n",
      "177/1900 (epoch 4), train_loss = 6.155, time/batch = 1.556\n",
      "178/1900 (epoch 4), train_loss = 6.134, time/batch = 1.726\n",
      "179/1900 (epoch 4), train_loss = 6.129, time/batch = 1.634\n",
      "180/1900 (epoch 4), train_loss = 6.117, time/batch = 1.634\n",
      "181/1900 (epoch 4), train_loss = 6.147, time/batch = 1.650\n",
      "182/1900 (epoch 4), train_loss = 6.078, time/batch = 1.808\n",
      "183/1900 (epoch 4), train_loss = 6.156, time/batch = 1.693\n",
      "184/1900 (epoch 4), train_loss = 6.154, time/batch = 1.726\n",
      "185/1900 (epoch 4), train_loss = 6.113, time/batch = 1.738\n",
      "186/1900 (epoch 4), train_loss = 6.115, time/batch = 1.641\n",
      "187/1900 (epoch 4), train_loss = 6.162, time/batch = 1.673\n",
      "188/1900 (epoch 4), train_loss = 6.142, time/batch = 1.583\n",
      "189/1900 (epoch 4), train_loss = 6.145, time/batch = 1.592\n",
      "190/1900 (epoch 5), train_loss = 6.195, time/batch = 1.560\n",
      "191/1900 (epoch 5), train_loss = 6.116, time/batch = 1.673\n",
      "192/1900 (epoch 5), train_loss = 6.127, time/batch = 1.520\n",
      "193/1900 (epoch 5), train_loss = 6.105, time/batch = 1.268\n",
      "194/1900 (epoch 5), train_loss = 6.137, time/batch = 1.504\n",
      "195/1900 (epoch 5), train_loss = 6.162, time/batch = 1.505\n",
      "196/1900 (epoch 5), train_loss = 6.124, time/batch = 1.403\n",
      "197/1900 (epoch 5), train_loss = 6.158, time/batch = 1.321\n",
      "198/1900 (epoch 5), train_loss = 6.134, time/batch = 1.279\n",
      "199/1900 (epoch 5), train_loss = 6.108, time/batch = 1.435\n",
      "200/1900 (epoch 5), train_loss = 6.105, time/batch = 1.431\n",
      "201/1900 (epoch 5), train_loss = 6.095, time/batch = 1.578\n",
      "202/1900 (epoch 5), train_loss = 6.116, time/batch = 1.339\n",
      "203/1900 (epoch 5), train_loss = 6.108, time/batch = 1.423\n",
      "204/1900 (epoch 5), train_loss = 6.107, time/batch = 1.433\n",
      "205/1900 (epoch 5), train_loss = 6.155, time/batch = 1.384\n",
      "206/1900 (epoch 5), train_loss = 6.147, time/batch = 1.343\n",
      "207/1900 (epoch 5), train_loss = 6.115, time/batch = 1.558\n",
      "208/1900 (epoch 5), train_loss = 6.123, time/batch = 1.558\n",
      "209/1900 (epoch 5), train_loss = 6.139, time/batch = 1.431\n",
      "210/1900 (epoch 5), train_loss = 6.140, time/batch = 1.279\n",
      "211/1900 (epoch 5), train_loss = 6.151, time/batch = 1.272\n",
      "212/1900 (epoch 5), train_loss = 6.096, time/batch = 1.189\n",
      "213/1900 (epoch 5), train_loss = 6.125, time/batch = 1.293\n",
      "214/1900 (epoch 5), train_loss = 6.163, time/batch = 1.269\n",
      "215/1900 (epoch 5), train_loss = 6.148, time/batch = 1.424\n",
      "216/1900 (epoch 5), train_loss = 6.131, time/batch = 1.439\n",
      "217/1900 (epoch 5), train_loss = 6.118, time/batch = 1.407\n",
      "218/1900 (epoch 5), train_loss = 6.110, time/batch = 1.170\n",
      "219/1900 (epoch 5), train_loss = 6.141, time/batch = 1.213\n",
      "220/1900 (epoch 5), train_loss = 6.069, time/batch = 1.214\n",
      "221/1900 (epoch 5), train_loss = 6.146, time/batch = 1.231\n",
      "222/1900 (epoch 5), train_loss = 6.144, time/batch = 1.332\n",
      "223/1900 (epoch 5), train_loss = 6.103, time/batch = 1.273\n",
      "224/1900 (epoch 5), train_loss = 6.102, time/batch = 1.359\n",
      "225/1900 (epoch 5), train_loss = 6.149, time/batch = 1.345\n",
      "226/1900 (epoch 5), train_loss = 6.132, time/batch = 1.299\n",
      "227/1900 (epoch 5), train_loss = 6.144, time/batch = 1.298\n",
      "228/1900 (epoch 6), train_loss = 6.191, time/batch = 1.262\n",
      "229/1900 (epoch 6), train_loss = 6.125, time/batch = 1.169\n",
      "230/1900 (epoch 6), train_loss = 6.121, time/batch = 1.412\n",
      "231/1900 (epoch 6), train_loss = 6.103, time/batch = 1.312\n",
      "232/1900 (epoch 6), train_loss = 6.135, time/batch = 1.395\n",
      "233/1900 (epoch 6), train_loss = 6.159, time/batch = 1.412\n",
      "234/1900 (epoch 6), train_loss = 6.125, time/batch = 1.232\n",
      "235/1900 (epoch 6), train_loss = 6.156, time/batch = 1.311\n",
      "236/1900 (epoch 6), train_loss = 6.132, time/batch = 1.423\n",
      "237/1900 (epoch 6), train_loss = 6.105, time/batch = 1.231\n",
      "238/1900 (epoch 6), train_loss = 6.099, time/batch = 1.392\n",
      "239/1900 (epoch 6), train_loss = 6.093, time/batch = 1.412\n",
      "240/1900 (epoch 6), train_loss = 6.116, time/batch = 1.439\n",
      "241/1900 (epoch 6), train_loss = 6.104, time/batch = 1.353\n",
      "242/1900 (epoch 6), train_loss = 6.106, time/batch = 1.338\n",
      "243/1900 (epoch 6), train_loss = 6.150, time/batch = 1.408\n",
      "244/1900 (epoch 6), train_loss = 6.149, time/batch = 1.334\n",
      "245/1900 (epoch 6), train_loss = 6.116, time/batch = 1.417\n",
      "246/1900 (epoch 6), train_loss = 6.114, time/batch = 1.395\n",
      "247/1900 (epoch 6), train_loss = 6.135, time/batch = 1.376\n",
      "248/1900 (epoch 6), train_loss = 6.143, time/batch = 1.228\n",
      "249/1900 (epoch 6), train_loss = 6.145, time/batch = 1.356\n",
      "250/1900 (epoch 6), train_loss = 6.093, time/batch = 1.267\n",
      "251/1900 (epoch 6), train_loss = 6.121, time/batch = 1.414\n",
      "252/1900 (epoch 6), train_loss = 6.160, time/batch = 1.308\n",
      "253/1900 (epoch 6), train_loss = 6.143, time/batch = 1.410\n",
      "254/1900 (epoch 6), train_loss = 6.127, time/batch = 1.409\n",
      "255/1900 (epoch 6), train_loss = 6.112, time/batch = 1.423\n",
      "256/1900 (epoch 6), train_loss = 6.107, time/batch = 1.222\n",
      "257/1900 (epoch 6), train_loss = 6.141, time/batch = 1.247\n",
      "258/1900 (epoch 6), train_loss = 6.066, time/batch = 1.406\n",
      "259/1900 (epoch 6), train_loss = 6.141, time/batch = 1.566\n",
      "260/1900 (epoch 6), train_loss = 6.143, time/batch = 1.668\n",
      "261/1900 (epoch 6), train_loss = 6.105, time/batch = 1.624\n",
      "262/1900 (epoch 6), train_loss = 6.105, time/batch = 1.508\n",
      "263/1900 (epoch 6), train_loss = 6.148, time/batch = 1.791\n",
      "264/1900 (epoch 6), train_loss = 6.134, time/batch = 1.736\n",
      "265/1900 (epoch 6), train_loss = 6.144, time/batch = 1.764\n",
      "266/1900 (epoch 7), train_loss = 6.178, time/batch = 1.756\n",
      "267/1900 (epoch 7), train_loss = 6.124, time/batch = 1.614\n",
      "268/1900 (epoch 7), train_loss = 6.123, time/batch = 1.586\n",
      "269/1900 (epoch 7), train_loss = 6.103, time/batch = 1.537\n",
      "270/1900 (epoch 7), train_loss = 6.136, time/batch = 1.718\n",
      "271/1900 (epoch 7), train_loss = 6.157, time/batch = 1.696\n",
      "272/1900 (epoch 7), train_loss = 6.122, time/batch = 1.558\n",
      "273/1900 (epoch 7), train_loss = 6.158, time/batch = 1.562\n",
      "274/1900 (epoch 7), train_loss = 6.133, time/batch = 1.655\n",
      "275/1900 (epoch 7), train_loss = 6.100, time/batch = 1.733\n",
      "276/1900 (epoch 7), train_loss = 6.102, time/batch = 1.259\n",
      "277/1900 (epoch 7), train_loss = 6.092, time/batch = 1.306\n",
      "278/1900 (epoch 7), train_loss = 6.116, time/batch = 1.411\n",
      "279/1900 (epoch 7), train_loss = 6.103, time/batch = 1.341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280/1900 (epoch 7), train_loss = 6.110, time/batch = 1.259\n",
      "281/1900 (epoch 7), train_loss = 6.151, time/batch = 1.453\n",
      "282/1900 (epoch 7), train_loss = 6.145, time/batch = 1.473\n",
      "283/1900 (epoch 7), train_loss = 6.108, time/batch = 1.259\n",
      "284/1900 (epoch 7), train_loss = 6.117, time/batch = 1.517\n",
      "285/1900 (epoch 7), train_loss = 6.134, time/batch = 1.408\n",
      "286/1900 (epoch 7), train_loss = 6.141, time/batch = 1.436\n",
      "287/1900 (epoch 7), train_loss = 6.145, time/batch = 1.576\n",
      "288/1900 (epoch 7), train_loss = 6.092, time/batch = 1.655\n",
      "289/1900 (epoch 7), train_loss = 6.118, time/batch = 1.657\n",
      "290/1900 (epoch 7), train_loss = 6.156, time/batch = 1.598\n",
      "291/1900 (epoch 7), train_loss = 6.138, time/batch = 1.506\n",
      "292/1900 (epoch 7), train_loss = 6.124, time/batch = 1.505\n",
      "293/1900 (epoch 7), train_loss = 6.114, time/batch = 1.338\n",
      "294/1900 (epoch 7), train_loss = 6.107, time/batch = 1.709\n",
      "295/1900 (epoch 7), train_loss = 6.136, time/batch = 1.294\n",
      "296/1900 (epoch 7), train_loss = 6.065, time/batch = 1.681\n",
      "297/1900 (epoch 7), train_loss = 6.143, time/batch = 1.334\n",
      "298/1900 (epoch 7), train_loss = 6.141, time/batch = 1.397\n",
      "299/1900 (epoch 7), train_loss = 6.107, time/batch = 1.360\n",
      "300/1900 (epoch 7), train_loss = 6.103, time/batch = 1.369\n",
      "301/1900 (epoch 7), train_loss = 6.144, time/batch = 1.333\n",
      "302/1900 (epoch 7), train_loss = 6.130, time/batch = 1.492\n",
      "303/1900 (epoch 7), train_loss = 6.143, time/batch = 1.441\n",
      "304/1900 (epoch 8), train_loss = 6.164, time/batch = 1.455\n",
      "305/1900 (epoch 8), train_loss = 6.121, time/batch = 1.323\n",
      "306/1900 (epoch 8), train_loss = 6.122, time/batch = 1.290\n",
      "307/1900 (epoch 8), train_loss = 6.101, time/batch = 1.300\n",
      "308/1900 (epoch 8), train_loss = 6.133, time/batch = 1.225\n",
      "309/1900 (epoch 8), train_loss = 6.158, time/batch = 1.289\n",
      "310/1900 (epoch 8), train_loss = 6.123, time/batch = 1.281\n",
      "311/1900 (epoch 8), train_loss = 6.156, time/batch = 1.387\n",
      "312/1900 (epoch 8), train_loss = 6.130, time/batch = 1.397\n",
      "313/1900 (epoch 8), train_loss = 6.105, time/batch = 1.483\n",
      "314/1900 (epoch 8), train_loss = 6.096, time/batch = 1.321\n",
      "315/1900 (epoch 8), train_loss = 6.089, time/batch = 1.358\n",
      "316/1900 (epoch 8), train_loss = 6.113, time/batch = 1.417\n",
      "317/1900 (epoch 8), train_loss = 6.103, time/batch = 1.320\n",
      "318/1900 (epoch 8), train_loss = 6.108, time/batch = 1.336\n",
      "319/1900 (epoch 8), train_loss = 6.153, time/batch = 1.277\n",
      "320/1900 (epoch 8), train_loss = 6.142, time/batch = 1.345\n",
      "321/1900 (epoch 8), train_loss = 6.110, time/batch = 1.336\n",
      "322/1900 (epoch 8), train_loss = 6.117, time/batch = 1.232\n",
      "323/1900 (epoch 8), train_loss = 6.133, time/batch = 1.507\n",
      "324/1900 (epoch 8), train_loss = 6.138, time/batch = 1.353\n",
      "325/1900 (epoch 8), train_loss = 6.148, time/batch = 1.323\n",
      "326/1900 (epoch 8), train_loss = 6.091, time/batch = 1.316\n",
      "327/1900 (epoch 8), train_loss = 6.119, time/batch = 1.330\n",
      "328/1900 (epoch 8), train_loss = 6.152, time/batch = 1.309\n",
      "329/1900 (epoch 8), train_loss = 6.145, time/batch = 1.248\n",
      "330/1900 (epoch 8), train_loss = 6.124, time/batch = 1.277\n",
      "331/1900 (epoch 8), train_loss = 6.109, time/batch = 1.360\n",
      "332/1900 (epoch 8), train_loss = 6.105, time/batch = 1.256\n",
      "333/1900 (epoch 8), train_loss = 6.136, time/batch = 1.374\n",
      "334/1900 (epoch 8), train_loss = 6.062, time/batch = 1.445\n",
      "335/1900 (epoch 8), train_loss = 6.141, time/batch = 1.373\n",
      "336/1900 (epoch 8), train_loss = 6.142, time/batch = 1.398\n",
      "337/1900 (epoch 8), train_loss = 6.101, time/batch = 1.317\n",
      "338/1900 (epoch 8), train_loss = 6.098, time/batch = 1.462\n",
      "339/1900 (epoch 8), train_loss = 6.146, time/batch = 1.599\n",
      "340/1900 (epoch 8), train_loss = 6.128, time/batch = 1.625\n",
      "341/1900 (epoch 8), train_loss = 6.141, time/batch = 1.616\n",
      "342/1900 (epoch 9), train_loss = 6.153, time/batch = 1.684\n",
      "343/1900 (epoch 9), train_loss = 6.112, time/batch = 1.777\n",
      "344/1900 (epoch 9), train_loss = 6.117, time/batch = 1.367\n",
      "345/1900 (epoch 9), train_loss = 6.102, time/batch = 1.378\n",
      "346/1900 (epoch 9), train_loss = 6.132, time/batch = 1.381\n",
      "347/1900 (epoch 9), train_loss = 6.155, time/batch = 1.555\n",
      "348/1900 (epoch 9), train_loss = 6.118, time/batch = 1.979\n",
      "349/1900 (epoch 9), train_loss = 6.153, time/batch = 1.551\n",
      "350/1900 (epoch 9), train_loss = 6.129, time/batch = 1.358\n",
      "351/1900 (epoch 9), train_loss = 6.099, time/batch = 1.277\n",
      "352/1900 (epoch 9), train_loss = 6.098, time/batch = 1.279\n",
      "353/1900 (epoch 9), train_loss = 6.083, time/batch = 1.387\n",
      "354/1900 (epoch 9), train_loss = 6.115, time/batch = 1.356\n",
      "355/1900 (epoch 9), train_loss = 6.102, time/batch = 1.220\n",
      "356/1900 (epoch 9), train_loss = 6.106, time/batch = 1.324\n",
      "357/1900 (epoch 9), train_loss = 6.150, time/batch = 1.209\n",
      "358/1900 (epoch 9), train_loss = 6.145, time/batch = 1.206\n",
      "359/1900 (epoch 9), train_loss = 6.111, time/batch = 1.459\n",
      "360/1900 (epoch 9), train_loss = 6.115, time/batch = 1.803\n",
      "361/1900 (epoch 9), train_loss = 6.131, time/batch = 1.898\n",
      "362/1900 (epoch 9), train_loss = 6.139, time/batch = 1.627\n",
      "363/1900 (epoch 9), train_loss = 6.142, time/batch = 1.796\n",
      "364/1900 (epoch 9), train_loss = 6.091, time/batch = 1.834\n",
      "365/1900 (epoch 9), train_loss = 6.120, time/batch = 1.837\n",
      "366/1900 (epoch 9), train_loss = 6.154, time/batch = 1.891\n",
      "367/1900 (epoch 9), train_loss = 6.141, time/batch = 1.793\n",
      "368/1900 (epoch 9), train_loss = 6.121, time/batch = 1.900\n",
      "369/1900 (epoch 9), train_loss = 6.112, time/batch = 1.870\n",
      "370/1900 (epoch 9), train_loss = 6.102, time/batch = 1.703\n",
      "371/1900 (epoch 9), train_loss = 6.134, time/batch = 1.725\n",
      "372/1900 (epoch 9), train_loss = 6.065, time/batch = 1.566\n",
      "373/1900 (epoch 9), train_loss = 6.139, time/batch = 1.685\n",
      "374/1900 (epoch 9), train_loss = 6.140, time/batch = 1.564\n",
      "375/1900 (epoch 9), train_loss = 6.100, time/batch = 1.588\n",
      "376/1900 (epoch 9), train_loss = 6.098, time/batch = 1.418\n",
      "377/1900 (epoch 9), train_loss = 6.146, time/batch = 1.458\n",
      "378/1900 (epoch 9), train_loss = 6.124, time/batch = 1.552\n",
      "379/1900 (epoch 9), train_loss = 6.143, time/batch = 1.619\n",
      "380/1900 (epoch 10), train_loss = 6.155, time/batch = 1.513\n",
      "381/1900 (epoch 10), train_loss = 6.112, time/batch = 1.507\n",
      "382/1900 (epoch 10), train_loss = 6.113, time/batch = 1.539\n",
      "383/1900 (epoch 10), train_loss = 6.103, time/batch = 1.535\n",
      "384/1900 (epoch 10), train_loss = 6.130, time/batch = 1.334\n",
      "385/1900 (epoch 10), train_loss = 6.149, time/batch = 1.414\n",
      "386/1900 (epoch 10), train_loss = 6.116, time/batch = 1.360\n",
      "387/1900 (epoch 10), train_loss = 6.151, time/batch = 1.485\n",
      "388/1900 (epoch 10), train_loss = 6.128, time/batch = 1.786\n",
      "389/1900 (epoch 10), train_loss = 6.098, time/batch = 1.656\n",
      "390/1900 (epoch 10), train_loss = 6.101, time/batch = 1.739\n",
      "391/1900 (epoch 10), train_loss = 6.082, time/batch = 1.751\n",
      "392/1900 (epoch 10), train_loss = 6.114, time/batch = 1.741\n",
      "393/1900 (epoch 10), train_loss = 6.101, time/batch = 1.263\n",
      "394/1900 (epoch 10), train_loss = 6.103, time/batch = 1.313\n",
      "395/1900 (epoch 10), train_loss = 6.147, time/batch = 1.420\n",
      "396/1900 (epoch 10), train_loss = 6.141, time/batch = 1.348\n",
      "397/1900 (epoch 10), train_loss = 6.108, time/batch = 1.495\n",
      "398/1900 (epoch 10), train_loss = 6.111, time/batch = 1.379\n",
      "399/1900 (epoch 10), train_loss = 6.131, time/batch = 1.355\n",
      "400/1900 (epoch 10), train_loss = 6.137, time/batch = 1.283\n",
      "401/1900 (epoch 10), train_loss = 6.141, time/batch = 1.316\n",
      "402/1900 (epoch 10), train_loss = 6.090, time/batch = 1.319\n",
      "403/1900 (epoch 10), train_loss = 6.118, time/batch = 1.397\n",
      "404/1900 (epoch 10), train_loss = 6.152, time/batch = 1.493\n",
      "405/1900 (epoch 10), train_loss = 6.138, time/batch = 1.242\n",
      "406/1900 (epoch 10), train_loss = 6.120, time/batch = 1.286\n",
      "407/1900 (epoch 10), train_loss = 6.110, time/batch = 1.270\n",
      "408/1900 (epoch 10), train_loss = 6.100, time/batch = 1.339\n",
      "409/1900 (epoch 10), train_loss = 6.132, time/batch = 1.407\n",
      "410/1900 (epoch 10), train_loss = 6.063, time/batch = 1.245\n",
      "411/1900 (epoch 10), train_loss = 6.135, time/batch = 1.305\n",
      "412/1900 (epoch 10), train_loss = 6.140, time/batch = 1.392\n",
      "413/1900 (epoch 10), train_loss = 6.099, time/batch = 1.255\n",
      "414/1900 (epoch 10), train_loss = 6.097, time/batch = 1.364\n",
      "415/1900 (epoch 10), train_loss = 6.142, time/batch = 1.360\n",
      "416/1900 (epoch 10), train_loss = 6.124, time/batch = 1.439\n",
      "417/1900 (epoch 10), train_loss = 6.139, time/batch = 1.378\n",
      "418/1900 (epoch 11), train_loss = 6.148, time/batch = 1.353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419/1900 (epoch 11), train_loss = 6.111, time/batch = 1.279\n",
      "420/1900 (epoch 11), train_loss = 6.113, time/batch = 1.441\n",
      "421/1900 (epoch 11), train_loss = 6.099, time/batch = 1.607\n",
      "422/1900 (epoch 11), train_loss = 6.128, time/batch = 1.685\n",
      "423/1900 (epoch 11), train_loss = 6.150, time/batch = 1.347\n",
      "424/1900 (epoch 11), train_loss = 6.115, time/batch = 1.376\n",
      "425/1900 (epoch 11), train_loss = 6.153, time/batch = 1.402\n",
      "426/1900 (epoch 11), train_loss = 6.125, time/batch = 1.476\n",
      "427/1900 (epoch 11), train_loss = 6.100, time/batch = 1.438\n",
      "428/1900 (epoch 11), train_loss = 6.097, time/batch = 1.290\n",
      "429/1900 (epoch 11), train_loss = 6.083, time/batch = 1.247\n",
      "430/1900 (epoch 11), train_loss = 6.109, time/batch = 1.277\n",
      "431/1900 (epoch 11), train_loss = 6.099, time/batch = 1.421\n",
      "432/1900 (epoch 11), train_loss = 6.100, time/batch = 1.348\n",
      "433/1900 (epoch 11), train_loss = 6.146, time/batch = 1.333\n",
      "434/1900 (epoch 11), train_loss = 6.141, time/batch = 1.383\n",
      "435/1900 (epoch 11), train_loss = 6.105, time/batch = 1.261\n",
      "436/1900 (epoch 11), train_loss = 6.114, time/batch = 1.349\n",
      "437/1900 (epoch 11), train_loss = 6.130, time/batch = 1.355\n",
      "438/1900 (epoch 11), train_loss = 6.137, time/batch = 1.411\n",
      "439/1900 (epoch 11), train_loss = 6.142, time/batch = 1.210\n",
      "440/1900 (epoch 11), train_loss = 6.087, time/batch = 1.302\n",
      "441/1900 (epoch 11), train_loss = 6.115, time/batch = 1.467\n",
      "442/1900 (epoch 11), train_loss = 6.149, time/batch = 1.426\n",
      "443/1900 (epoch 11), train_loss = 6.140, time/batch = 1.396\n",
      "444/1900 (epoch 11), train_loss = 6.117, time/batch = 1.227\n",
      "445/1900 (epoch 11), train_loss = 6.106, time/batch = 1.336\n",
      "446/1900 (epoch 11), train_loss = 6.099, time/batch = 1.247\n",
      "447/1900 (epoch 11), train_loss = 6.131, time/batch = 1.215\n",
      "448/1900 (epoch 11), train_loss = 6.063, time/batch = 1.331\n",
      "449/1900 (epoch 11), train_loss = 6.137, time/batch = 1.275\n",
      "450/1900 (epoch 11), train_loss = 6.135, time/batch = 1.327\n",
      "451/1900 (epoch 11), train_loss = 6.098, time/batch = 1.369\n",
      "452/1900 (epoch 11), train_loss = 6.096, time/batch = 1.400\n",
      "453/1900 (epoch 11), train_loss = 6.142, time/batch = 1.651\n",
      "454/1900 (epoch 11), train_loss = 6.124, time/batch = 1.602\n",
      "455/1900 (epoch 11), train_loss = 6.138, time/batch = 1.452\n",
      "456/1900 (epoch 12), train_loss = 6.141, time/batch = 1.296\n",
      "457/1900 (epoch 12), train_loss = 6.110, time/batch = 1.282\n",
      "458/1900 (epoch 12), train_loss = 6.111, time/batch = 1.277\n",
      "459/1900 (epoch 12), train_loss = 6.099, time/batch = 1.368\n",
      "460/1900 (epoch 12), train_loss = 6.126, time/batch = 1.373\n",
      "461/1900 (epoch 12), train_loss = 6.144, time/batch = 1.283\n",
      "462/1900 (epoch 12), train_loss = 6.115, time/batch = 1.449\n",
      "463/1900 (epoch 12), train_loss = 6.149, time/batch = 1.348\n",
      "464/1900 (epoch 12), train_loss = 6.125, time/batch = 1.272\n",
      "465/1900 (epoch 12), train_loss = 6.096, time/batch = 1.400\n",
      "466/1900 (epoch 12), train_loss = 6.093, time/batch = 1.470\n",
      "467/1900 (epoch 12), train_loss = 6.082, time/batch = 1.369\n",
      "468/1900 (epoch 12), train_loss = 6.111, time/batch = 1.460\n",
      "469/1900 (epoch 12), train_loss = 6.094, time/batch = 1.335\n",
      "470/1900 (epoch 12), train_loss = 6.098, time/batch = 1.425\n",
      "471/1900 (epoch 12), train_loss = 6.146, time/batch = 1.393\n",
      "472/1900 (epoch 12), train_loss = 6.142, time/batch = 1.271\n",
      "473/1900 (epoch 12), train_loss = 6.105, time/batch = 1.357\n",
      "474/1900 (epoch 12), train_loss = 6.113, time/batch = 1.613\n",
      "475/1900 (epoch 12), train_loss = 6.131, time/batch = 1.472\n",
      "476/1900 (epoch 12), train_loss = 6.132, time/batch = 1.624\n",
      "477/1900 (epoch 12), train_loss = 6.140, time/batch = 1.325\n",
      "478/1900 (epoch 12), train_loss = 6.090, time/batch = 1.424\n",
      "479/1900 (epoch 12), train_loss = 6.114, time/batch = 1.673\n",
      "480/1900 (epoch 12), train_loss = 6.148, time/batch = 1.623\n",
      "481/1900 (epoch 12), train_loss = 6.135, time/batch = 1.627\n",
      "482/1900 (epoch 12), train_loss = 6.122, time/batch = 1.397\n",
      "483/1900 (epoch 12), train_loss = 6.106, time/batch = 1.488\n",
      "484/1900 (epoch 12), train_loss = 6.098, time/batch = 1.305\n",
      "485/1900 (epoch 12), train_loss = 6.126, time/batch = 1.346\n",
      "486/1900 (epoch 12), train_loss = 6.060, time/batch = 1.337\n",
      "487/1900 (epoch 12), train_loss = 6.138, time/batch = 1.366\n",
      "488/1900 (epoch 12), train_loss = 6.136, time/batch = 1.324\n",
      "489/1900 (epoch 12), train_loss = 6.096, time/batch = 1.271\n",
      "490/1900 (epoch 12), train_loss = 6.093, time/batch = 1.414\n",
      "491/1900 (epoch 12), train_loss = 6.140, time/batch = 1.595\n",
      "492/1900 (epoch 12), train_loss = 6.124, time/batch = 1.395\n",
      "493/1900 (epoch 12), train_loss = 6.138, time/batch = 1.416\n",
      "494/1900 (epoch 13), train_loss = 6.138, time/batch = 1.361\n",
      "495/1900 (epoch 13), train_loss = 6.108, time/batch = 1.414\n",
      "496/1900 (epoch 13), train_loss = 6.117, time/batch = 1.221\n",
      "497/1900 (epoch 13), train_loss = 6.096, time/batch = 1.385\n",
      "498/1900 (epoch 13), train_loss = 6.123, time/batch = 1.444\n",
      "499/1900 (epoch 13), train_loss = 6.149, time/batch = 1.566\n",
      "500/1900 (epoch 13), train_loss = 6.114, time/batch = 1.330\n",
      "501/1900 (epoch 13), train_loss = 6.145, time/batch = 1.322\n",
      "502/1900 (epoch 13), train_loss = 6.120, time/batch = 1.388\n",
      "503/1900 (epoch 13), train_loss = 6.096, time/batch = 1.396\n",
      "504/1900 (epoch 13), train_loss = 6.092, time/batch = 1.321\n",
      "505/1900 (epoch 13), train_loss = 6.080, time/batch = 1.299\n",
      "506/1900 (epoch 13), train_loss = 6.108, time/batch = 1.456\n",
      "507/1900 (epoch 13), train_loss = 6.096, time/batch = 1.496\n",
      "508/1900 (epoch 13), train_loss = 6.101, time/batch = 1.597\n",
      "509/1900 (epoch 13), train_loss = 6.142, time/batch = 1.731\n",
      "510/1900 (epoch 13), train_loss = 6.137, time/batch = 1.909\n",
      "511/1900 (epoch 13), train_loss = 6.103, time/batch = 1.619\n",
      "512/1900 (epoch 13), train_loss = 6.108, time/batch = 1.463\n",
      "513/1900 (epoch 13), train_loss = 6.125, time/batch = 1.457\n",
      "514/1900 (epoch 13), train_loss = 6.133, time/batch = 1.405\n",
      "515/1900 (epoch 13), train_loss = 6.137, time/batch = 1.249\n",
      "516/1900 (epoch 13), train_loss = 6.083, time/batch = 1.263\n",
      "517/1900 (epoch 13), train_loss = 6.112, time/batch = 1.337\n",
      "518/1900 (epoch 13), train_loss = 6.146, time/batch = 1.313\n",
      "519/1900 (epoch 13), train_loss = 6.137, time/batch = 1.399\n",
      "520/1900 (epoch 13), train_loss = 6.119, time/batch = 1.331\n",
      "521/1900 (epoch 13), train_loss = 6.103, time/batch = 1.298\n",
      "522/1900 (epoch 13), train_loss = 6.098, time/batch = 1.331\n",
      "523/1900 (epoch 13), train_loss = 6.129, time/batch = 1.426\n",
      "524/1900 (epoch 13), train_loss = 6.058, time/batch = 1.255\n",
      "525/1900 (epoch 13), train_loss = 6.132, time/batch = 1.258\n",
      "526/1900 (epoch 13), train_loss = 6.130, time/batch = 1.243\n",
      "527/1900 (epoch 13), train_loss = 6.096, time/batch = 1.237\n",
      "528/1900 (epoch 13), train_loss = 6.090, time/batch = 1.225\n",
      "529/1900 (epoch 13), train_loss = 6.137, time/batch = 1.255\n",
      "530/1900 (epoch 13), train_loss = 6.122, time/batch = 1.226\n",
      "531/1900 (epoch 13), train_loss = 6.136, time/batch = 1.255\n",
      "532/1900 (epoch 14), train_loss = 6.136, time/batch = 1.256\n",
      "533/1900 (epoch 14), train_loss = 6.105, time/batch = 1.243\n",
      "534/1900 (epoch 14), train_loss = 6.108, time/batch = 1.249\n",
      "535/1900 (epoch 14), train_loss = 6.099, time/batch = 1.256\n",
      "536/1900 (epoch 14), train_loss = 6.124, time/batch = 1.382\n",
      "537/1900 (epoch 14), train_loss = 6.144, time/batch = 1.282\n",
      "538/1900 (epoch 14), train_loss = 6.114, time/batch = 1.331\n",
      "539/1900 (epoch 14), train_loss = 6.151, time/batch = 1.309\n",
      "540/1900 (epoch 14), train_loss = 6.120, time/batch = 1.256\n",
      "541/1900 (epoch 14), train_loss = 6.093, time/batch = 1.280\n",
      "542/1900 (epoch 14), train_loss = 6.090, time/batch = 1.273\n",
      "543/1900 (epoch 14), train_loss = 6.081, time/batch = 1.270\n",
      "544/1900 (epoch 14), train_loss = 6.106, time/batch = 1.286\n",
      "545/1900 (epoch 14), train_loss = 6.092, time/batch = 1.604\n",
      "546/1900 (epoch 14), train_loss = 6.097, time/batch = 1.611\n",
      "547/1900 (epoch 14), train_loss = 6.140, time/batch = 1.329\n",
      "548/1900 (epoch 14), train_loss = 6.138, time/batch = 1.261\n",
      "549/1900 (epoch 14), train_loss = 6.103, time/batch = 1.329\n",
      "550/1900 (epoch 14), train_loss = 6.108, time/batch = 1.379\n",
      "551/1900 (epoch 14), train_loss = 6.130, time/batch = 1.320\n",
      "552/1900 (epoch 14), train_loss = 6.131, time/batch = 1.287\n",
      "553/1900 (epoch 14), train_loss = 6.136, time/batch = 1.437\n",
      "554/1900 (epoch 14), train_loss = 6.083, time/batch = 1.512\n",
      "555/1900 (epoch 14), train_loss = 6.111, time/batch = 1.489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "556/1900 (epoch 14), train_loss = 6.146, time/batch = 1.495\n",
      "557/1900 (epoch 14), train_loss = 6.134, time/batch = 1.751\n",
      "558/1900 (epoch 14), train_loss = 6.115, time/batch = 1.768\n",
      "559/1900 (epoch 14), train_loss = 6.108, time/batch = 1.458\n",
      "560/1900 (epoch 14), train_loss = 6.097, time/batch = 1.229\n",
      "561/1900 (epoch 14), train_loss = 6.126, time/batch = 1.307\n",
      "562/1900 (epoch 14), train_loss = 6.057, time/batch = 1.379\n",
      "563/1900 (epoch 14), train_loss = 6.131, time/batch = 1.364\n",
      "564/1900 (epoch 14), train_loss = 6.133, time/batch = 1.333\n",
      "565/1900 (epoch 14), train_loss = 6.093, time/batch = 1.357\n",
      "566/1900 (epoch 14), train_loss = 6.093, time/batch = 1.356\n",
      "567/1900 (epoch 14), train_loss = 6.137, time/batch = 1.340\n",
      "568/1900 (epoch 14), train_loss = 6.121, time/batch = 1.494\n",
      "569/1900 (epoch 14), train_loss = 6.137, time/batch = 1.757\n",
      "570/1900 (epoch 15), train_loss = 6.139, time/batch = 1.585\n",
      "571/1900 (epoch 15), train_loss = 6.104, time/batch = 1.462\n",
      "572/1900 (epoch 15), train_loss = 6.109, time/batch = 1.474\n",
      "573/1900 (epoch 15), train_loss = 6.095, time/batch = 1.709\n",
      "574/1900 (epoch 15), train_loss = 6.125, time/batch = 1.555\n",
      "575/1900 (epoch 15), train_loss = 6.146, time/batch = 1.797\n",
      "576/1900 (epoch 15), train_loss = 6.112, time/batch = 1.612\n",
      "577/1900 (epoch 15), train_loss = 6.144, time/batch = 1.551\n",
      "578/1900 (epoch 15), train_loss = 6.118, time/batch = 1.279\n",
      "579/1900 (epoch 15), train_loss = 6.090, time/batch = 1.256\n",
      "580/1900 (epoch 15), train_loss = 6.093, time/batch = 1.217\n",
      "581/1900 (epoch 15), train_loss = 6.077, time/batch = 1.238\n",
      "582/1900 (epoch 15), train_loss = 6.105, time/batch = 1.179\n",
      "583/1900 (epoch 15), train_loss = 6.093, time/batch = 1.392\n",
      "584/1900 (epoch 15), train_loss = 6.098, time/batch = 1.688\n",
      "585/1900 (epoch 15), train_loss = 6.139, time/batch = 1.606\n",
      "586/1900 (epoch 15), train_loss = 6.139, time/batch = 1.563\n",
      "587/1900 (epoch 15), train_loss = 6.106, time/batch = 1.506\n",
      "588/1900 (epoch 15), train_loss = 6.107, time/batch = 1.306\n",
      "589/1900 (epoch 15), train_loss = 6.127, time/batch = 1.296\n",
      "590/1900 (epoch 15), train_loss = 6.127, time/batch = 1.221\n",
      "591/1900 (epoch 15), train_loss = 6.135, time/batch = 1.271\n",
      "592/1900 (epoch 15), train_loss = 6.084, time/batch = 1.305\n",
      "593/1900 (epoch 15), train_loss = 6.110, time/batch = 1.387\n",
      "594/1900 (epoch 15), train_loss = 6.141, time/batch = 1.509\n",
      "595/1900 (epoch 15), train_loss = 6.131, time/batch = 1.383\n",
      "596/1900 (epoch 15), train_loss = 6.116, time/batch = 1.411\n",
      "597/1900 (epoch 15), train_loss = 6.102, time/batch = 1.356\n",
      "598/1900 (epoch 15), train_loss = 6.097, time/batch = 1.300\n",
      "599/1900 (epoch 15), train_loss = 6.125, time/batch = 1.322\n",
      "600/1900 (epoch 15), train_loss = 6.059, time/batch = 1.254\n",
      "601/1900 (epoch 15), train_loss = 6.129, time/batch = 1.219\n",
      "602/1900 (epoch 15), train_loss = 6.132, time/batch = 1.331\n",
      "603/1900 (epoch 15), train_loss = 6.091, time/batch = 1.280\n",
      "604/1900 (epoch 15), train_loss = 6.088, time/batch = 1.491\n",
      "605/1900 (epoch 15), train_loss = 6.134, time/batch = 1.417\n",
      "606/1900 (epoch 15), train_loss = 6.121, time/batch = 1.288\n",
      "607/1900 (epoch 15), train_loss = 6.132, time/batch = 1.323\n",
      "608/1900 (epoch 16), train_loss = 6.134, time/batch = 1.436\n",
      "609/1900 (epoch 16), train_loss = 6.116, time/batch = 1.497\n",
      "610/1900 (epoch 16), train_loss = 6.105, time/batch = 1.450\n",
      "611/1900 (epoch 16), train_loss = 6.092, time/batch = 1.237\n",
      "612/1900 (epoch 16), train_loss = 6.119, time/batch = 1.316\n",
      "613/1900 (epoch 16), train_loss = 6.143, time/batch = 1.296\n",
      "614/1900 (epoch 16), train_loss = 6.111, time/batch = 1.418\n",
      "615/1900 (epoch 16), train_loss = 6.147, time/batch = 1.415\n",
      "616/1900 (epoch 16), train_loss = 6.120, time/batch = 1.555\n",
      "617/1900 (epoch 16), train_loss = 6.092, time/batch = 1.691\n",
      "618/1900 (epoch 16), train_loss = 6.090, time/batch = 1.527\n",
      "619/1900 (epoch 16), train_loss = 6.079, time/batch = 1.573\n",
      "620/1900 (epoch 16), train_loss = 6.104, time/batch = 1.339\n",
      "621/1900 (epoch 16), train_loss = 6.093, time/batch = 1.406\n",
      "622/1900 (epoch 16), train_loss = 6.095, time/batch = 1.394\n",
      "623/1900 (epoch 16), train_loss = 6.138, time/batch = 1.339\n",
      "624/1900 (epoch 16), train_loss = 6.136, time/batch = 1.340\n",
      "625/1900 (epoch 16), train_loss = 6.099, time/batch = 1.294\n",
      "626/1900 (epoch 16), train_loss = 6.107, time/batch = 1.405\n",
      "627/1900 (epoch 16), train_loss = 6.125, time/batch = 1.337\n",
      "628/1900 (epoch 16), train_loss = 6.127, time/batch = 1.327\n",
      "629/1900 (epoch 16), train_loss = 6.137, time/batch = 1.453\n",
      "630/1900 (epoch 16), train_loss = 6.082, time/batch = 1.546\n",
      "631/1900 (epoch 16), train_loss = 6.109, time/batch = 1.430\n",
      "632/1900 (epoch 16), train_loss = 6.141, time/batch = 1.239\n",
      "633/1900 (epoch 16), train_loss = 6.132, time/batch = 1.277\n",
      "634/1900 (epoch 16), train_loss = 6.112, time/batch = 1.385\n",
      "635/1900 (epoch 16), train_loss = 6.102, time/batch = 1.291\n",
      "636/1900 (epoch 16), train_loss = 6.093, time/batch = 1.381\n",
      "637/1900 (epoch 16), train_loss = 6.126, time/batch = 1.449\n",
      "638/1900 (epoch 16), train_loss = 6.055, time/batch = 1.610\n",
      "639/1900 (epoch 16), train_loss = 6.133, time/batch = 1.364\n",
      "640/1900 (epoch 16), train_loss = 6.129, time/batch = 1.267\n",
      "641/1900 (epoch 16), train_loss = 6.092, time/batch = 1.296\n",
      "642/1900 (epoch 16), train_loss = 6.090, time/batch = 1.239\n",
      "643/1900 (epoch 16), train_loss = 6.135, time/batch = 1.292\n",
      "644/1900 (epoch 16), train_loss = 6.119, time/batch = 1.224\n",
      "645/1900 (epoch 16), train_loss = 6.133, time/batch = 1.257\n",
      "646/1900 (epoch 17), train_loss = 6.132, time/batch = 1.711\n",
      "647/1900 (epoch 17), train_loss = 6.116, time/batch = 1.734\n",
      "648/1900 (epoch 17), train_loss = 6.109, time/batch = 1.479\n",
      "649/1900 (epoch 17), train_loss = 6.092, time/batch = 1.455\n",
      "650/1900 (epoch 17), train_loss = 6.120, time/batch = 1.590\n",
      "651/1900 (epoch 17), train_loss = 6.142, time/batch = 1.605\n",
      "652/1900 (epoch 17), train_loss = 6.110, time/batch = 1.610\n",
      "653/1900 (epoch 17), train_loss = 6.147, time/batch = 1.527\n",
      "654/1900 (epoch 17), train_loss = 6.118, time/batch = 1.604\n",
      "655/1900 (epoch 17), train_loss = 6.089, time/batch = 1.582\n",
      "656/1900 (epoch 17), train_loss = 6.089, time/batch = 1.598\n",
      "657/1900 (epoch 17), train_loss = 6.077, time/batch = 1.259\n",
      "658/1900 (epoch 17), train_loss = 6.103, time/batch = 1.266\n",
      "659/1900 (epoch 17), train_loss = 6.093, time/batch = 1.602\n",
      "660/1900 (epoch 17), train_loss = 6.097, time/batch = 1.335\n",
      "661/1900 (epoch 17), train_loss = 6.138, time/batch = 1.330\n",
      "662/1900 (epoch 17), train_loss = 6.137, time/batch = 1.263\n",
      "663/1900 (epoch 17), train_loss = 6.101, time/batch = 1.376\n",
      "664/1900 (epoch 17), train_loss = 6.108, time/batch = 1.709\n",
      "665/1900 (epoch 17), train_loss = 6.124, time/batch = 1.407\n",
      "666/1900 (epoch 17), train_loss = 6.124, time/batch = 1.377\n",
      "667/1900 (epoch 17), train_loss = 6.135, time/batch = 1.483\n",
      "668/1900 (epoch 17), train_loss = 6.079, time/batch = 1.412\n",
      "669/1900 (epoch 17), train_loss = 6.109, time/batch = 1.716\n",
      "670/1900 (epoch 17), train_loss = 6.145, time/batch = 1.213\n",
      "671/1900 (epoch 17), train_loss = 6.133, time/batch = 1.244\n",
      "672/1900 (epoch 17), train_loss = 6.115, time/batch = 1.362\n",
      "673/1900 (epoch 17), train_loss = 6.104, time/batch = 1.292\n",
      "674/1900 (epoch 17), train_loss = 6.098, time/batch = 1.251\n",
      "675/1900 (epoch 17), train_loss = 6.126, time/batch = 1.315\n",
      "676/1900 (epoch 17), train_loss = 6.055, time/batch = 1.377\n",
      "677/1900 (epoch 17), train_loss = 6.128, time/batch = 1.295\n",
      "678/1900 (epoch 17), train_loss = 6.129, time/batch = 1.311\n",
      "679/1900 (epoch 17), train_loss = 6.091, time/batch = 1.537\n",
      "680/1900 (epoch 17), train_loss = 6.088, time/batch = 1.823\n",
      "681/1900 (epoch 17), train_loss = 6.133, time/batch = 1.313\n",
      "682/1900 (epoch 17), train_loss = 6.120, time/batch = 1.627\n",
      "683/1900 (epoch 17), train_loss = 6.131, time/batch = 1.734\n",
      "684/1900 (epoch 18), train_loss = 6.129, time/batch = 1.568\n",
      "685/1900 (epoch 18), train_loss = 6.112, time/batch = 1.642\n",
      "686/1900 (epoch 18), train_loss = 6.104, time/batch = 1.306\n",
      "687/1900 (epoch 18), train_loss = 6.092, time/batch = 1.244\n",
      "688/1900 (epoch 18), train_loss = 6.119, time/batch = 1.276\n",
      "689/1900 (epoch 18), train_loss = 6.137, time/batch = 1.495\n",
      "690/1900 (epoch 18), train_loss = 6.108, time/batch = 1.476\n",
      "691/1900 (epoch 18), train_loss = 6.142, time/batch = 1.518\n",
      "692/1900 (epoch 18), train_loss = 6.117, time/batch = 1.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "693/1900 (epoch 18), train_loss = 6.089, time/batch = 1.549\n",
      "694/1900 (epoch 18), train_loss = 6.088, time/batch = 1.532\n",
      "695/1900 (epoch 18), train_loss = 6.077, time/batch = 1.530\n",
      "696/1900 (epoch 18), train_loss = 6.102, time/batch = 1.504\n",
      "697/1900 (epoch 18), train_loss = 6.090, time/batch = 1.440\n",
      "698/1900 (epoch 18), train_loss = 6.095, time/batch = 1.669\n",
      "699/1900 (epoch 18), train_loss = 6.137, time/batch = 1.637\n",
      "700/1900 (epoch 18), train_loss = 6.136, time/batch = 1.634\n",
      "701/1900 (epoch 18), train_loss = 6.101, time/batch = 1.474\n",
      "702/1900 (epoch 18), train_loss = 6.107, time/batch = 1.619\n",
      "703/1900 (epoch 18), train_loss = 6.123, time/batch = 1.264\n",
      "704/1900 (epoch 18), train_loss = 6.124, time/batch = 1.274\n",
      "705/1900 (epoch 18), train_loss = 6.133, time/batch = 1.373\n",
      "706/1900 (epoch 18), train_loss = 6.077, time/batch = 1.384\n",
      "707/1900 (epoch 18), train_loss = 6.108, time/batch = 1.296\n",
      "708/1900 (epoch 18), train_loss = 6.142, time/batch = 1.354\n",
      "709/1900 (epoch 18), train_loss = 6.131, time/batch = 1.337\n",
      "710/1900 (epoch 18), train_loss = 6.110, time/batch = 1.200\n",
      "711/1900 (epoch 18), train_loss = 6.100, time/batch = 1.322\n",
      "712/1900 (epoch 18), train_loss = 6.093, time/batch = 1.249\n",
      "713/1900 (epoch 18), train_loss = 6.127, time/batch = 1.307\n",
      "714/1900 (epoch 18), train_loss = 6.054, time/batch = 1.288\n",
      "715/1900 (epoch 18), train_loss = 6.132, time/batch = 1.466\n",
      "716/1900 (epoch 18), train_loss = 6.127, time/batch = 1.371\n",
      "717/1900 (epoch 18), train_loss = 6.091, time/batch = 1.291\n",
      "718/1900 (epoch 18), train_loss = 6.085, time/batch = 1.366\n",
      "719/1900 (epoch 18), train_loss = 6.133, time/batch = 1.380\n",
      "720/1900 (epoch 18), train_loss = 6.117, time/batch = 1.289\n",
      "721/1900 (epoch 18), train_loss = 6.129, time/batch = 1.283\n",
      "722/1900 (epoch 19), train_loss = 6.132, time/batch = 1.302\n",
      "723/1900 (epoch 19), train_loss = 6.113, time/batch = 1.363\n",
      "724/1900 (epoch 19), train_loss = 6.106, time/batch = 1.292\n",
      "725/1900 (epoch 19), train_loss = 6.092, time/batch = 1.363\n",
      "726/1900 (epoch 19), train_loss = 6.121, time/batch = 1.443\n",
      "727/1900 (epoch 19), train_loss = 6.139, time/batch = 1.261\n",
      "728/1900 (epoch 19), train_loss = 6.109, time/batch = 1.293\n",
      "729/1900 (epoch 19), train_loss = 6.141, time/batch = 1.515\n",
      "730/1900 (epoch 19), train_loss = 6.113, time/batch = 1.505\n",
      "731/1900 (epoch 19), train_loss = 6.093, time/batch = 1.543\n",
      "732/1900 (epoch 19), train_loss = 6.091, time/batch = 1.492\n",
      "733/1900 (epoch 19), train_loss = 6.074, time/batch = 1.408\n",
      "734/1900 (epoch 19), train_loss = 6.101, time/batch = 1.429\n",
      "735/1900 (epoch 19), train_loss = 6.088, time/batch = 1.384\n",
      "736/1900 (epoch 19), train_loss = 6.093, time/batch = 1.427\n",
      "737/1900 (epoch 19), train_loss = 6.141, time/batch = 1.513\n",
      "738/1900 (epoch 19), train_loss = 6.134, time/batch = 1.724\n",
      "739/1900 (epoch 19), train_loss = 6.099, time/batch = 1.376\n",
      "740/1900 (epoch 19), train_loss = 6.105, time/batch = 1.351\n",
      "741/1900 (epoch 19), train_loss = 6.124, time/batch = 1.438\n",
      "742/1900 (epoch 19), train_loss = 6.123, time/batch = 1.602\n",
      "743/1900 (epoch 19), train_loss = 6.131, time/batch = 1.395\n",
      "744/1900 (epoch 19), train_loss = 6.081, time/batch = 1.590\n",
      "745/1900 (epoch 19), train_loss = 6.108, time/batch = 1.351\n",
      "746/1900 (epoch 19), train_loss = 6.142, time/batch = 1.261\n",
      "747/1900 (epoch 19), train_loss = 6.128, time/batch = 1.386\n",
      "748/1900 (epoch 19), train_loss = 6.113, time/batch = 1.441\n",
      "749/1900 (epoch 19), train_loss = 6.100, time/batch = 1.467\n",
      "750/1900 (epoch 19), train_loss = 6.095, time/batch = 1.554\n",
      "751/1900 (epoch 19), train_loss = 6.122, time/batch = 1.592\n",
      "752/1900 (epoch 19), train_loss = 6.057, time/batch = 1.499\n",
      "753/1900 (epoch 19), train_loss = 6.127, time/batch = 1.500\n",
      "754/1900 (epoch 19), train_loss = 6.125, time/batch = 1.352\n",
      "755/1900 (epoch 19), train_loss = 6.089, time/batch = 1.260\n",
      "756/1900 (epoch 19), train_loss = 6.087, time/batch = 1.259\n",
      "757/1900 (epoch 19), train_loss = 6.131, time/batch = 1.304\n",
      "758/1900 (epoch 19), train_loss = 6.117, time/batch = 1.409\n",
      "759/1900 (epoch 19), train_loss = 6.133, time/batch = 1.372\n",
      "760/1900 (epoch 20), train_loss = 6.129, time/batch = 1.510\n",
      "761/1900 (epoch 20), train_loss = 6.111, time/batch = 1.352\n",
      "762/1900 (epoch 20), train_loss = 6.103, time/batch = 1.193\n",
      "763/1900 (epoch 20), train_loss = 6.088, time/batch = 1.186\n",
      "764/1900 (epoch 20), train_loss = 6.120, time/batch = 1.247\n",
      "765/1900 (epoch 20), train_loss = 6.141, time/batch = 1.180\n",
      "766/1900 (epoch 20), train_loss = 6.109, time/batch = 1.290\n",
      "767/1900 (epoch 20), train_loss = 6.143, time/batch = 1.288\n",
      "768/1900 (epoch 20), train_loss = 6.114, time/batch = 1.287\n",
      "769/1900 (epoch 20), train_loss = 6.089, time/batch = 1.269\n",
      "770/1900 (epoch 20), train_loss = 6.091, time/batch = 1.314\n",
      "771/1900 (epoch 20), train_loss = 6.071, time/batch = 1.224\n",
      "772/1900 (epoch 20), train_loss = 6.101, time/batch = 1.311\n",
      "773/1900 (epoch 20), train_loss = 6.087, time/batch = 1.458\n",
      "774/1900 (epoch 20), train_loss = 6.092, time/batch = 1.357\n",
      "775/1900 (epoch 20), train_loss = 6.135, time/batch = 1.433\n",
      "776/1900 (epoch 20), train_loss = 6.137, time/batch = 1.165\n",
      "777/1900 (epoch 20), train_loss = 6.097, time/batch = 1.163\n",
      "778/1900 (epoch 20), train_loss = 6.101, time/batch = 1.166\n",
      "779/1900 (epoch 20), train_loss = 6.124, time/batch = 1.156\n",
      "780/1900 (epoch 20), train_loss = 6.125, time/batch = 1.269\n",
      "781/1900 (epoch 20), train_loss = 6.131, time/batch = 1.174\n",
      "782/1900 (epoch 20), train_loss = 6.080, time/batch = 1.201\n",
      "783/1900 (epoch 20), train_loss = 6.107, time/batch = 1.214\n",
      "784/1900 (epoch 20), train_loss = 6.136, time/batch = 1.308\n",
      "785/1900 (epoch 20), train_loss = 6.127, time/batch = 1.310\n",
      "786/1900 (epoch 20), train_loss = 6.111, time/batch = 1.259\n",
      "787/1900 (epoch 20), train_loss = 6.097, time/batch = 1.315\n",
      "788/1900 (epoch 20), train_loss = 6.089, time/batch = 1.326\n",
      "789/1900 (epoch 20), train_loss = 6.119, time/batch = 1.315\n",
      "790/1900 (epoch 20), train_loss = 6.054, time/batch = 1.241\n",
      "791/1900 (epoch 20), train_loss = 6.126, time/batch = 1.223\n",
      "792/1900 (epoch 20), train_loss = 6.124, time/batch = 1.246\n",
      "793/1900 (epoch 20), train_loss = 6.090, time/batch = 1.466\n",
      "794/1900 (epoch 20), train_loss = 6.084, time/batch = 1.347\n",
      "795/1900 (epoch 20), train_loss = 6.131, time/batch = 1.245\n",
      "796/1900 (epoch 20), train_loss = 6.115, time/batch = 1.330\n",
      "797/1900 (epoch 20), train_loss = 6.130, time/batch = 1.162\n",
      "798/1900 (epoch 21), train_loss = 6.130, time/batch = 1.343\n",
      "799/1900 (epoch 21), train_loss = 6.110, time/batch = 1.383\n",
      "800/1900 (epoch 21), train_loss = 6.102, time/batch = 1.161\n",
      "801/1900 (epoch 21), train_loss = 6.093, time/batch = 1.153\n",
      "802/1900 (epoch 21), train_loss = 6.119, time/batch = 1.216\n",
      "803/1900 (epoch 21), train_loss = 6.140, time/batch = 1.428\n",
      "804/1900 (epoch 21), train_loss = 6.105, time/batch = 1.188\n",
      "805/1900 (epoch 21), train_loss = 6.138, time/batch = 1.177\n",
      "806/1900 (epoch 21), train_loss = 6.113, time/batch = 1.401\n",
      "807/1900 (epoch 21), train_loss = 6.089, time/batch = 1.339\n",
      "808/1900 (epoch 21), train_loss = 6.089, time/batch = 1.462\n",
      "809/1900 (epoch 21), train_loss = 6.072, time/batch = 1.514\n",
      "810/1900 (epoch 21), train_loss = 6.100, time/batch = 1.493\n",
      "811/1900 (epoch 21), train_loss = 6.089, time/batch = 1.410\n",
      "812/1900 (epoch 21), train_loss = 6.088, time/batch = 1.394\n",
      "813/1900 (epoch 21), train_loss = 6.137, time/batch = 1.301\n",
      "814/1900 (epoch 21), train_loss = 6.132, time/batch = 1.470\n",
      "815/1900 (epoch 21), train_loss = 6.097, time/batch = 1.655\n",
      "816/1900 (epoch 21), train_loss = 6.101, time/batch = 1.718\n",
      "817/1900 (epoch 21), train_loss = 6.126, time/batch = 1.735\n",
      "818/1900 (epoch 21), train_loss = 6.121, time/batch = 1.589\n",
      "819/1900 (epoch 21), train_loss = 6.130, time/batch = 1.529\n",
      "820/1900 (epoch 21), train_loss = 6.075, time/batch = 1.314\n",
      "821/1900 (epoch 21), train_loss = 6.106, time/batch = 1.395\n",
      "822/1900 (epoch 21), train_loss = 6.137, time/batch = 1.185\n",
      "823/1900 (epoch 21), train_loss = 6.127, time/batch = 1.177\n",
      "824/1900 (epoch 21), train_loss = 6.111, time/batch = 1.178\n",
      "825/1900 (epoch 21), train_loss = 6.097, time/batch = 1.173\n",
      "826/1900 (epoch 21), train_loss = 6.091, time/batch = 1.188\n",
      "827/1900 (epoch 21), train_loss = 6.120, time/batch = 1.166\n",
      "828/1900 (epoch 21), train_loss = 6.051, time/batch = 1.165\n",
      "829/1900 (epoch 21), train_loss = 6.129, time/batch = 1.379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "830/1900 (epoch 21), train_loss = 6.124, time/batch = 1.569\n",
      "831/1900 (epoch 21), train_loss = 6.089, time/batch = 1.558\n",
      "832/1900 (epoch 21), train_loss = 6.085, time/batch = 1.395\n",
      "833/1900 (epoch 21), train_loss = 6.130, time/batch = 1.414\n",
      "834/1900 (epoch 21), train_loss = 6.113, time/batch = 1.262\n",
      "835/1900 (epoch 21), train_loss = 6.127, time/batch = 1.434\n",
      "836/1900 (epoch 22), train_loss = 6.128, time/batch = 1.267\n",
      "837/1900 (epoch 22), train_loss = 6.109, time/batch = 1.292\n",
      "838/1900 (epoch 22), train_loss = 6.104, time/batch = 1.387\n",
      "839/1900 (epoch 22), train_loss = 6.087, time/batch = 1.334\n",
      "840/1900 (epoch 22), train_loss = 6.115, time/batch = 1.216\n",
      "841/1900 (epoch 22), train_loss = 6.137, time/batch = 1.315\n",
      "842/1900 (epoch 22), train_loss = 6.106, time/batch = 1.270\n",
      "843/1900 (epoch 22), train_loss = 6.139, time/batch = 1.336\n",
      "844/1900 (epoch 22), train_loss = 6.115, time/batch = 1.395\n",
      "845/1900 (epoch 22), train_loss = 6.087, time/batch = 1.266\n",
      "846/1900 (epoch 22), train_loss = 6.087, time/batch = 1.411\n",
      "847/1900 (epoch 22), train_loss = 6.072, time/batch = 1.442\n",
      "848/1900 (epoch 22), train_loss = 6.098, time/batch = 1.422\n",
      "849/1900 (epoch 22), train_loss = 6.087, time/batch = 1.284\n",
      "850/1900 (epoch 22), train_loss = 6.093, time/batch = 1.333\n",
      "851/1900 (epoch 22), train_loss = 6.133, time/batch = 1.367\n",
      "852/1900 (epoch 22), train_loss = 6.132, time/batch = 1.308\n",
      "853/1900 (epoch 22), train_loss = 6.096, time/batch = 1.384\n",
      "854/1900 (epoch 22), train_loss = 6.101, time/batch = 1.400\n",
      "855/1900 (epoch 22), train_loss = 6.119, time/batch = 1.390\n",
      "856/1900 (epoch 22), train_loss = 6.121, time/batch = 1.404\n",
      "857/1900 (epoch 22), train_loss = 6.131, time/batch = 1.398\n",
      "858/1900 (epoch 22), train_loss = 6.075, time/batch = 1.485\n",
      "859/1900 (epoch 22), train_loss = 6.105, time/batch = 1.218\n",
      "860/1900 (epoch 22), train_loss = 6.136, time/batch = 1.390\n",
      "861/1900 (epoch 22), train_loss = 6.127, time/batch = 1.320\n",
      "862/1900 (epoch 22), train_loss = 6.111, time/batch = 1.296\n",
      "863/1900 (epoch 22), train_loss = 6.100, time/batch = 1.293\n",
      "864/1900 (epoch 22), train_loss = 6.090, time/batch = 1.301\n",
      "865/1900 (epoch 22), train_loss = 6.120, time/batch = 1.171\n",
      "866/1900 (epoch 22), train_loss = 6.054, time/batch = 1.162\n",
      "867/1900 (epoch 22), train_loss = 6.128, time/batch = 1.185\n",
      "868/1900 (epoch 22), train_loss = 6.124, time/batch = 1.426\n",
      "869/1900 (epoch 22), train_loss = 6.089, time/batch = 1.165\n",
      "870/1900 (epoch 22), train_loss = 6.081, time/batch = 1.189\n",
      "871/1900 (epoch 22), train_loss = 6.129, time/batch = 1.225\n",
      "872/1900 (epoch 22), train_loss = 6.114, time/batch = 1.193\n",
      "873/1900 (epoch 22), train_loss = 6.126, time/batch = 1.178\n",
      "874/1900 (epoch 23), train_loss = 6.125, time/batch = 1.185\n",
      "875/1900 (epoch 23), train_loss = 6.108, time/batch = 1.204\n",
      "876/1900 (epoch 23), train_loss = 6.104, time/batch = 1.174\n",
      "877/1900 (epoch 23), train_loss = 6.088, time/batch = 1.174\n",
      "878/1900 (epoch 23), train_loss = 6.118, time/batch = 1.168\n",
      "879/1900 (epoch 23), train_loss = 6.138, time/batch = 1.242\n",
      "880/1900 (epoch 23), train_loss = 6.104, time/batch = 1.166\n",
      "881/1900 (epoch 23), train_loss = 6.138, time/batch = 1.173\n",
      "882/1900 (epoch 23), train_loss = 6.113, time/batch = 1.215\n",
      "883/1900 (epoch 23), train_loss = 6.084, time/batch = 1.168\n",
      "884/1900 (epoch 23), train_loss = 6.085, time/batch = 1.178\n",
      "885/1900 (epoch 23), train_loss = 6.071, time/batch = 1.174\n",
      "886/1900 (epoch 23), train_loss = 6.100, time/batch = 1.227\n",
      "887/1900 (epoch 23), train_loss = 6.087, time/batch = 1.181\n",
      "888/1900 (epoch 23), train_loss = 6.092, time/batch = 1.383\n",
      "889/1900 (epoch 23), train_loss = 6.132, time/batch = 1.644\n",
      "890/1900 (epoch 23), train_loss = 6.131, time/batch = 1.620\n",
      "891/1900 (epoch 23), train_loss = 6.095, time/batch = 1.741\n",
      "892/1900 (epoch 23), train_loss = 6.100, time/batch = 1.627\n",
      "893/1900 (epoch 23), train_loss = 6.122, time/batch = 1.368\n",
      "894/1900 (epoch 23), train_loss = 6.122, time/batch = 1.406\n",
      "895/1900 (epoch 23), train_loss = 6.132, time/batch = 1.556\n",
      "896/1900 (epoch 23), train_loss = 6.074, time/batch = 1.380\n",
      "897/1900 (epoch 23), train_loss = 6.103, time/batch = 1.166\n",
      "898/1900 (epoch 23), train_loss = 6.134, time/batch = 1.348\n",
      "899/1900 (epoch 23), train_loss = 6.126, time/batch = 1.160\n",
      "900/1900 (epoch 23), train_loss = 6.112, time/batch = 1.160\n",
      "901/1900 (epoch 23), train_loss = 6.096, time/batch = 1.156\n",
      "902/1900 (epoch 23), train_loss = 6.089, time/batch = 1.277\n",
      "903/1900 (epoch 23), train_loss = 6.120, time/batch = 1.392\n",
      "904/1900 (epoch 23), train_loss = 6.053, time/batch = 1.297\n",
      "905/1900 (epoch 23), train_loss = 6.126, time/batch = 1.449\n",
      "906/1900 (epoch 23), train_loss = 6.124, time/batch = 1.397\n",
      "907/1900 (epoch 23), train_loss = 6.085, time/batch = 1.440\n",
      "908/1900 (epoch 23), train_loss = 6.082, time/batch = 1.336\n",
      "909/1900 (epoch 23), train_loss = 6.129, time/batch = 1.411\n",
      "910/1900 (epoch 23), train_loss = 6.116, time/batch = 1.357\n",
      "911/1900 (epoch 23), train_loss = 6.129, time/batch = 1.312\n",
      "912/1900 (epoch 24), train_loss = 6.120, time/batch = 1.382\n",
      "913/1900 (epoch 24), train_loss = 6.107, time/batch = 1.325\n",
      "914/1900 (epoch 24), train_loss = 6.098, time/batch = 1.369\n",
      "915/1900 (epoch 24), train_loss = 6.086, time/batch = 1.395\n",
      "916/1900 (epoch 24), train_loss = 6.115, time/batch = 1.330\n",
      "917/1900 (epoch 24), train_loss = 6.138, time/batch = 1.366\n",
      "918/1900 (epoch 24), train_loss = 6.105, time/batch = 1.353\n",
      "919/1900 (epoch 24), train_loss = 6.139, time/batch = 1.394\n",
      "920/1900 (epoch 24), train_loss = 6.114, time/batch = 1.224\n",
      "921/1900 (epoch 24), train_loss = 6.088, time/batch = 1.314\n",
      "922/1900 (epoch 24), train_loss = 6.087, time/batch = 1.300\n",
      "923/1900 (epoch 24), train_loss = 6.070, time/batch = 1.260\n",
      "924/1900 (epoch 24), train_loss = 6.096, time/batch = 1.239\n",
      "925/1900 (epoch 24), train_loss = 6.087, time/batch = 1.337\n",
      "926/1900 (epoch 24), train_loss = 6.090, time/batch = 1.304\n",
      "927/1900 (epoch 24), train_loss = 6.132, time/batch = 1.256\n",
      "928/1900 (epoch 24), train_loss = 6.128, time/batch = 1.292\n",
      "929/1900 (epoch 24), train_loss = 6.096, time/batch = 1.333\n",
      "930/1900 (epoch 24), train_loss = 6.103, time/batch = 1.280\n",
      "931/1900 (epoch 24), train_loss = 6.120, time/batch = 1.314\n",
      "932/1900 (epoch 24), train_loss = 6.123, time/batch = 1.288\n",
      "933/1900 (epoch 24), train_loss = 6.132, time/batch = 1.322\n",
      "934/1900 (epoch 24), train_loss = 6.076, time/batch = 1.234\n",
      "935/1900 (epoch 24), train_loss = 6.103, time/batch = 1.221\n",
      "936/1900 (epoch 24), train_loss = 6.137, time/batch = 1.311\n",
      "937/1900 (epoch 24), train_loss = 6.125, time/batch = 1.309\n",
      "938/1900 (epoch 24), train_loss = 6.108, time/batch = 1.462\n",
      "939/1900 (epoch 24), train_loss = 6.096, time/batch = 1.530\n",
      "940/1900 (epoch 24), train_loss = 6.088, time/batch = 1.861\n",
      "941/1900 (epoch 24), train_loss = 6.120, time/batch = 1.551\n",
      "942/1900 (epoch 24), train_loss = 6.051, time/batch = 1.438\n",
      "943/1900 (epoch 24), train_loss = 6.126, time/batch = 1.449\n",
      "944/1900 (epoch 24), train_loss = 6.122, time/batch = 1.571\n",
      "945/1900 (epoch 24), train_loss = 6.088, time/batch = 1.509\n",
      "946/1900 (epoch 24), train_loss = 6.082, time/batch = 1.837\n",
      "947/1900 (epoch 24), train_loss = 6.127, time/batch = 1.508\n",
      "948/1900 (epoch 24), train_loss = 6.111, time/batch = 1.454\n",
      "949/1900 (epoch 24), train_loss = 6.126, time/batch = 1.269\n",
      "950/1900 (epoch 25), train_loss = 6.121, time/batch = 1.386\n",
      "951/1900 (epoch 25), train_loss = 6.109, time/batch = 1.524\n",
      "952/1900 (epoch 25), train_loss = 6.100, time/batch = 1.301\n",
      "953/1900 (epoch 25), train_loss = 6.088, time/batch = 1.278\n",
      "954/1900 (epoch 25), train_loss = 6.115, time/batch = 1.250\n",
      "955/1900 (epoch 25), train_loss = 6.136, time/batch = 1.253\n",
      "956/1900 (epoch 25), train_loss = 6.102, time/batch = 1.274\n",
      "957/1900 (epoch 25), train_loss = 6.138, time/batch = 1.241\n",
      "958/1900 (epoch 25), train_loss = 6.111, time/batch = 1.236\n",
      "959/1900 (epoch 25), train_loss = 6.087, time/batch = 1.239\n",
      "960/1900 (epoch 25), train_loss = 6.085, time/batch = 1.249\n",
      "961/1900 (epoch 25), train_loss = 6.070, time/batch = 1.251\n",
      "962/1900 (epoch 25), train_loss = 6.096, time/batch = 1.246\n",
      "963/1900 (epoch 25), train_loss = 6.088, time/batch = 1.244\n",
      "964/1900 (epoch 25), train_loss = 6.089, time/batch = 1.262\n",
      "965/1900 (epoch 25), train_loss = 6.132, time/batch = 1.238\n",
      "966/1900 (epoch 25), train_loss = 6.130, time/batch = 1.274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "967/1900 (epoch 25), train_loss = 6.093, time/batch = 1.262\n",
      "968/1900 (epoch 25), train_loss = 6.101, time/batch = 1.246\n",
      "969/1900 (epoch 25), train_loss = 6.120, time/batch = 1.243\n",
      "970/1900 (epoch 25), train_loss = 6.121, time/batch = 1.257\n",
      "971/1900 (epoch 25), train_loss = 6.129, time/batch = 1.243\n",
      "972/1900 (epoch 25), train_loss = 6.074, time/batch = 1.281\n",
      "973/1900 (epoch 25), train_loss = 6.103, time/batch = 1.283\n",
      "974/1900 (epoch 25), train_loss = 6.136, time/batch = 1.246\n",
      "975/1900 (epoch 25), train_loss = 6.125, time/batch = 1.252\n",
      "976/1900 (epoch 25), train_loss = 6.109, time/batch = 1.251\n",
      "977/1900 (epoch 25), train_loss = 6.095, time/batch = 1.252\n",
      "978/1900 (epoch 25), train_loss = 6.090, time/batch = 1.231\n",
      "979/1900 (epoch 25), train_loss = 6.118, time/batch = 1.242\n",
      "980/1900 (epoch 25), train_loss = 6.053, time/batch = 1.264\n",
      "981/1900 (epoch 25), train_loss = 6.124, time/batch = 1.242\n",
      "982/1900 (epoch 25), train_loss = 6.122, time/batch = 1.232\n",
      "983/1900 (epoch 25), train_loss = 6.083, time/batch = 1.253\n",
      "984/1900 (epoch 25), train_loss = 6.081, time/batch = 1.273\n",
      "985/1900 (epoch 25), train_loss = 6.128, time/batch = 1.238\n",
      "986/1900 (epoch 25), train_loss = 6.112, time/batch = 1.243\n",
      "987/1900 (epoch 25), train_loss = 6.124, time/batch = 1.241\n",
      "988/1900 (epoch 26), train_loss = 6.123, time/batch = 1.276\n",
      "989/1900 (epoch 26), train_loss = 6.107, time/batch = 1.246\n",
      "990/1900 (epoch 26), train_loss = 6.100, time/batch = 1.237\n",
      "991/1900 (epoch 26), train_loss = 6.086, time/batch = 1.247\n",
      "992/1900 (epoch 26), train_loss = 6.112, time/batch = 1.237\n",
      "993/1900 (epoch 26), train_loss = 6.139, time/batch = 1.246\n",
      "994/1900 (epoch 26), train_loss = 6.103, time/batch = 1.249\n",
      "995/1900 (epoch 26), train_loss = 6.138, time/batch = 1.245\n",
      "996/1900 (epoch 26), train_loss = 6.112, time/batch = 1.289\n",
      "997/1900 (epoch 26), train_loss = 6.085, time/batch = 1.278\n",
      "998/1900 (epoch 26), train_loss = 6.085, time/batch = 1.306\n",
      "999/1900 (epoch 26), train_loss = 6.069, time/batch = 1.281\n",
      "1000/1900 (epoch 26), train_loss = 6.096, time/batch = 1.250\n",
      "model saved to ./save\\model.ckpt\n",
      "1001/1900 (epoch 26), train_loss = 6.084, time/batch = 1.348\n",
      "1002/1900 (epoch 26), train_loss = 6.089, time/batch = 1.374\n",
      "1003/1900 (epoch 26), train_loss = 6.131, time/batch = 1.337\n",
      "1004/1900 (epoch 26), train_loss = 6.131, time/batch = 1.261\n",
      "1005/1900 (epoch 26), train_loss = 6.093, time/batch = 1.311\n",
      "1006/1900 (epoch 26), train_loss = 6.099, time/batch = 1.266\n",
      "1007/1900 (epoch 26), train_loss = 6.117, time/batch = 1.250\n",
      "1008/1900 (epoch 26), train_loss = 6.121, time/batch = 1.259\n",
      "1009/1900 (epoch 26), train_loss = 6.128, time/batch = 1.258\n",
      "1010/1900 (epoch 26), train_loss = 6.073, time/batch = 1.261\n",
      "1011/1900 (epoch 26), train_loss = 6.103, time/batch = 1.267\n",
      "1012/1900 (epoch 26), train_loss = 6.133, time/batch = 1.251\n",
      "1013/1900 (epoch 26), train_loss = 6.123, time/batch = 1.251\n",
      "1014/1900 (epoch 26), train_loss = 6.107, time/batch = 1.313\n",
      "1015/1900 (epoch 26), train_loss = 6.093, time/batch = 1.345\n",
      "1016/1900 (epoch 26), train_loss = 6.088, time/batch = 1.252\n",
      "1017/1900 (epoch 26), train_loss = 6.118, time/batch = 1.237\n",
      "1018/1900 (epoch 26), train_loss = 6.050, time/batch = 1.251\n",
      "1019/1900 (epoch 26), train_loss = 6.124, time/batch = 1.269\n",
      "1020/1900 (epoch 26), train_loss = 6.120, time/batch = 1.241\n",
      "1021/1900 (epoch 26), train_loss = 6.086, time/batch = 1.252\n",
      "1022/1900 (epoch 26), train_loss = 6.081, time/batch = 1.273\n",
      "1023/1900 (epoch 26), train_loss = 6.124, time/batch = 1.246\n",
      "1024/1900 (epoch 26), train_loss = 6.108, time/batch = 1.250\n",
      "1025/1900 (epoch 26), train_loss = 6.119, time/batch = 1.273\n",
      "1026/1900 (epoch 27), train_loss = 6.117, time/batch = 1.304\n",
      "1027/1900 (epoch 27), train_loss = 6.104, time/batch = 1.275\n",
      "1028/1900 (epoch 27), train_loss = 6.099, time/batch = 1.262\n",
      "1029/1900 (epoch 27), train_loss = 6.083, time/batch = 1.247\n",
      "1030/1900 (epoch 27), train_loss = 6.113, time/batch = 1.268\n",
      "1031/1900 (epoch 27), train_loss = 6.133, time/batch = 1.263\n",
      "1032/1900 (epoch 27), train_loss = 6.099, time/batch = 1.258\n",
      "1033/1900 (epoch 27), train_loss = 6.131, time/batch = 1.259\n",
      "1034/1900 (epoch 27), train_loss = 6.106, time/batch = 1.252\n",
      "1035/1900 (epoch 27), train_loss = 6.080, time/batch = 1.249\n",
      "1036/1900 (epoch 27), train_loss = 6.081, time/batch = 1.255\n",
      "1037/1900 (epoch 27), train_loss = 6.064, time/batch = 1.256\n",
      "1038/1900 (epoch 27), train_loss = 6.089, time/batch = 1.260\n",
      "1039/1900 (epoch 27), train_loss = 6.080, time/batch = 1.250\n",
      "1040/1900 (epoch 27), train_loss = 6.080, time/batch = 1.257\n",
      "1041/1900 (epoch 27), train_loss = 6.125, time/batch = 1.260\n",
      "1042/1900 (epoch 27), train_loss = 6.122, time/batch = 1.273\n",
      "1043/1900 (epoch 27), train_loss = 6.087, time/batch = 1.263\n",
      "1044/1900 (epoch 27), train_loss = 6.091, time/batch = 1.243\n",
      "1045/1900 (epoch 27), train_loss = 6.108, time/batch = 1.244\n",
      "1046/1900 (epoch 27), train_loss = 6.108, time/batch = 1.243\n",
      "1047/1900 (epoch 27), train_loss = 6.117, time/batch = 1.251\n",
      "1048/1900 (epoch 27), train_loss = 6.061, time/batch = 1.255\n",
      "1049/1900 (epoch 27), train_loss = 6.089, time/batch = 1.258\n",
      "1050/1900 (epoch 27), train_loss = 6.119, time/batch = 1.258\n",
      "1051/1900 (epoch 27), train_loss = 6.110, time/batch = 1.270\n",
      "1052/1900 (epoch 27), train_loss = 6.093, time/batch = 1.264\n",
      "1053/1900 (epoch 27), train_loss = 6.076, time/batch = 1.259\n",
      "1054/1900 (epoch 27), train_loss = 6.070, time/batch = 1.272\n",
      "1055/1900 (epoch 27), train_loss = 6.099, time/batch = 1.264\n",
      "1056/1900 (epoch 27), train_loss = 6.032, time/batch = 1.260\n",
      "1057/1900 (epoch 27), train_loss = 6.102, time/batch = 1.251\n",
      "1058/1900 (epoch 27), train_loss = 6.100, time/batch = 1.257\n",
      "1059/1900 (epoch 27), train_loss = 6.059, time/batch = 1.273\n",
      "1060/1900 (epoch 27), train_loss = 6.053, time/batch = 1.261\n",
      "1061/1900 (epoch 27), train_loss = 6.101, time/batch = 1.264\n",
      "1062/1900 (epoch 27), train_loss = 6.082, time/batch = 1.266\n",
      "1063/1900 (epoch 27), train_loss = 6.094, time/batch = 1.264\n",
      "1064/1900 (epoch 28), train_loss = 6.096, time/batch = 1.261\n",
      "1065/1900 (epoch 28), train_loss = 6.078, time/batch = 1.254\n",
      "1066/1900 (epoch 28), train_loss = 6.069, time/batch = 1.240\n",
      "1067/1900 (epoch 28), train_loss = 6.050, time/batch = 1.258\n",
      "1068/1900 (epoch 28), train_loss = 6.077, time/batch = 1.258\n",
      "1069/1900 (epoch 28), train_loss = 6.096, time/batch = 1.260\n",
      "1070/1900 (epoch 28), train_loss = 6.066, time/batch = 1.254\n",
      "1071/1900 (epoch 28), train_loss = 6.096, time/batch = 1.264\n",
      "1072/1900 (epoch 28), train_loss = 6.069, time/batch = 1.251\n",
      "1073/1900 (epoch 28), train_loss = 6.034, time/batch = 1.247\n",
      "1074/1900 (epoch 28), train_loss = 6.037, time/batch = 1.255\n",
      "1075/1900 (epoch 28), train_loss = 6.018, time/batch = 1.272\n",
      "1076/1900 (epoch 28), train_loss = 6.043, time/batch = 1.263\n",
      "1077/1900 (epoch 28), train_loss = 6.034, time/batch = 1.259\n",
      "1078/1900 (epoch 28), train_loss = 6.038, time/batch = 1.248\n",
      "1079/1900 (epoch 28), train_loss = 6.083, time/batch = 1.256\n",
      "1080/1900 (epoch 28), train_loss = 6.078, time/batch = 1.268\n",
      "1081/1900 (epoch 28), train_loss = 6.036, time/batch = 1.272\n",
      "1082/1900 (epoch 28), train_loss = 6.042, time/batch = 1.271\n",
      "1083/1900 (epoch 28), train_loss = 6.056, time/batch = 1.265\n",
      "1084/1900 (epoch 28), train_loss = 6.057, time/batch = 1.271\n",
      "1085/1900 (epoch 28), train_loss = 6.064, time/batch = 1.273\n",
      "1086/1900 (epoch 28), train_loss = 6.010, time/batch = 1.260\n",
      "1087/1900 (epoch 28), train_loss = 6.035, time/batch = 1.270\n",
      "1088/1900 (epoch 28), train_loss = 6.063, time/batch = 1.245\n",
      "1089/1900 (epoch 28), train_loss = 6.055, time/batch = 1.281\n",
      "1090/1900 (epoch 28), train_loss = 6.033, time/batch = 1.256\n",
      "1091/1900 (epoch 28), train_loss = 6.022, time/batch = 1.243\n",
      "1092/1900 (epoch 28), train_loss = 6.016, time/batch = 1.271\n",
      "1093/1900 (epoch 28), train_loss = 6.041, time/batch = 1.366\n",
      "1094/1900 (epoch 28), train_loss = 5.970, time/batch = 1.273\n",
      "1095/1900 (epoch 28), train_loss = 6.044, time/batch = 1.248\n",
      "1096/1900 (epoch 28), train_loss = 6.035, time/batch = 1.250\n",
      "1097/1900 (epoch 28), train_loss = 6.006, time/batch = 1.265\n",
      "1098/1900 (epoch 28), train_loss = 5.992, time/batch = 1.266\n",
      "1099/1900 (epoch 28), train_loss = 6.039, time/batch = 1.271\n",
      "1100/1900 (epoch 28), train_loss = 6.021, time/batch = 1.259\n",
      "1101/1900 (epoch 28), train_loss = 6.031, time/batch = 1.256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1102/1900 (epoch 29), train_loss = 6.060, time/batch = 1.276\n",
      "1103/1900 (epoch 29), train_loss = 6.021, time/batch = 1.249\n",
      "1104/1900 (epoch 29), train_loss = 6.013, time/batch = 1.255\n",
      "1105/1900 (epoch 29), train_loss = 5.994, time/batch = 1.259\n",
      "1106/1900 (epoch 29), train_loss = 6.018, time/batch = 1.255\n",
      "1107/1900 (epoch 29), train_loss = 6.035, time/batch = 1.261\n",
      "1108/1900 (epoch 29), train_loss = 6.001, time/batch = 1.283\n",
      "1109/1900 (epoch 29), train_loss = 6.038, time/batch = 1.335\n",
      "1110/1900 (epoch 29), train_loss = 6.006, time/batch = 1.282\n",
      "1111/1900 (epoch 29), train_loss = 5.971, time/batch = 1.247\n",
      "1112/1900 (epoch 29), train_loss = 5.976, time/batch = 1.253\n",
      "1113/1900 (epoch 29), train_loss = 5.957, time/batch = 1.269\n",
      "1114/1900 (epoch 29), train_loss = 5.983, time/batch = 1.256\n",
      "1115/1900 (epoch 29), train_loss = 5.975, time/batch = 1.250\n",
      "1116/1900 (epoch 29), train_loss = 5.984, time/batch = 1.279\n",
      "1117/1900 (epoch 29), train_loss = 6.022, time/batch = 1.263\n",
      "1118/1900 (epoch 29), train_loss = 6.024, time/batch = 1.265\n",
      "1119/1900 (epoch 29), train_loss = 5.976, time/batch = 1.248\n",
      "1120/1900 (epoch 29), train_loss = 5.980, time/batch = 1.263\n",
      "1121/1900 (epoch 29), train_loss = 5.995, time/batch = 1.258\n",
      "1122/1900 (epoch 29), train_loss = 6.001, time/batch = 1.245\n",
      "1123/1900 (epoch 29), train_loss = 6.008, time/batch = 1.255\n",
      "1124/1900 (epoch 29), train_loss = 5.956, time/batch = 1.268\n",
      "1125/1900 (epoch 29), train_loss = 5.982, time/batch = 1.260\n",
      "1126/1900 (epoch 29), train_loss = 6.005, time/batch = 1.257\n",
      "1127/1900 (epoch 29), train_loss = 5.994, time/batch = 1.250\n",
      "1128/1900 (epoch 29), train_loss = 5.976, time/batch = 1.249\n",
      "1129/1900 (epoch 29), train_loss = 5.960, time/batch = 1.252\n",
      "1130/1900 (epoch 29), train_loss = 5.962, time/batch = 1.249\n",
      "1131/1900 (epoch 29), train_loss = 5.981, time/batch = 1.266\n",
      "1132/1900 (epoch 29), train_loss = 5.912, time/batch = 1.278\n",
      "1133/1900 (epoch 29), train_loss = 5.986, time/batch = 1.288\n",
      "1134/1900 (epoch 29), train_loss = 5.974, time/batch = 1.256\n",
      "1135/1900 (epoch 29), train_loss = 5.945, time/batch = 1.261\n",
      "1136/1900 (epoch 29), train_loss = 5.941, time/batch = 1.284\n",
      "1137/1900 (epoch 29), train_loss = 5.979, time/batch = 1.280\n",
      "1138/1900 (epoch 29), train_loss = 5.963, time/batch = 1.239\n",
      "1139/1900 (epoch 29), train_loss = 5.975, time/batch = 1.260\n",
      "1140/1900 (epoch 30), train_loss = 6.017, time/batch = 1.304\n",
      "1141/1900 (epoch 30), train_loss = 5.964, time/batch = 1.253\n",
      "1142/1900 (epoch 30), train_loss = 5.960, time/batch = 1.263\n",
      "1143/1900 (epoch 30), train_loss = 5.943, time/batch = 1.268\n",
      "1144/1900 (epoch 30), train_loss = 5.962, time/batch = 1.269\n",
      "1145/1900 (epoch 30), train_loss = 5.974, time/batch = 1.246\n",
      "1146/1900 (epoch 30), train_loss = 5.941, time/batch = 1.249\n",
      "1147/1900 (epoch 30), train_loss = 5.978, time/batch = 1.252\n",
      "1148/1900 (epoch 30), train_loss = 5.943, time/batch = 1.254\n",
      "1149/1900 (epoch 30), train_loss = 5.910, time/batch = 1.252\n",
      "1150/1900 (epoch 30), train_loss = 5.918, time/batch = 1.261\n",
      "1151/1900 (epoch 30), train_loss = 5.897, time/batch = 1.243\n",
      "1152/1900 (epoch 30), train_loss = 5.926, time/batch = 1.239\n",
      "1153/1900 (epoch 30), train_loss = 5.914, time/batch = 1.269\n",
      "1154/1900 (epoch 30), train_loss = 5.924, time/batch = 1.241\n",
      "1155/1900 (epoch 30), train_loss = 5.961, time/batch = 1.246\n",
      "1156/1900 (epoch 30), train_loss = 5.962, time/batch = 1.276\n",
      "1157/1900 (epoch 30), train_loss = 5.912, time/batch = 1.252\n",
      "1158/1900 (epoch 30), train_loss = 5.914, time/batch = 1.260\n",
      "1159/1900 (epoch 30), train_loss = 5.934, time/batch = 1.236\n",
      "1160/1900 (epoch 30), train_loss = 5.937, time/batch = 1.250\n",
      "1161/1900 (epoch 30), train_loss = 5.945, time/batch = 1.245\n",
      "1162/1900 (epoch 30), train_loss = 5.891, time/batch = 1.244\n",
      "1163/1900 (epoch 30), train_loss = 5.923, time/batch = 1.250\n",
      "1164/1900 (epoch 30), train_loss = 5.941, time/batch = 1.275\n",
      "1165/1900 (epoch 30), train_loss = 5.931, time/batch = 1.331\n",
      "1166/1900 (epoch 30), train_loss = 5.916, time/batch = 1.316\n",
      "1167/1900 (epoch 30), train_loss = 5.903, time/batch = 1.254\n",
      "1168/1900 (epoch 30), train_loss = 5.904, time/batch = 1.249\n",
      "1169/1900 (epoch 30), train_loss = 5.918, time/batch = 1.252\n",
      "1170/1900 (epoch 30), train_loss = 5.850, time/batch = 1.247\n",
      "1171/1900 (epoch 30), train_loss = 5.920, time/batch = 1.245\n",
      "1172/1900 (epoch 30), train_loss = 5.913, time/batch = 1.275\n",
      "1173/1900 (epoch 30), train_loss = 5.883, time/batch = 1.262\n",
      "1174/1900 (epoch 30), train_loss = 5.875, time/batch = 1.250\n",
      "1175/1900 (epoch 30), train_loss = 5.918, time/batch = 1.253\n",
      "1176/1900 (epoch 30), train_loss = 5.901, time/batch = 1.262\n",
      "1177/1900 (epoch 30), train_loss = 5.916, time/batch = 1.266\n",
      "1178/1900 (epoch 31), train_loss = 5.957, time/batch = 1.271\n",
      "1179/1900 (epoch 31), train_loss = 5.903, time/batch = 1.319\n",
      "1180/1900 (epoch 31), train_loss = 5.901, time/batch = 1.267\n",
      "1181/1900 (epoch 31), train_loss = 5.879, time/batch = 1.241\n",
      "1182/1900 (epoch 31), train_loss = 5.901, time/batch = 1.256\n",
      "1183/1900 (epoch 31), train_loss = 5.917, time/batch = 1.233\n",
      "1184/1900 (epoch 31), train_loss = 5.872, time/batch = 1.252\n",
      "1185/1900 (epoch 31), train_loss = 5.917, time/batch = 1.245\n",
      "1186/1900 (epoch 31), train_loss = 5.880, time/batch = 1.250\n",
      "1187/1900 (epoch 31), train_loss = 5.851, time/batch = 1.238\n",
      "1188/1900 (epoch 31), train_loss = 5.856, time/batch = 1.270\n",
      "1189/1900 (epoch 31), train_loss = 5.835, time/batch = 1.249\n",
      "1190/1900 (epoch 31), train_loss = 5.863, time/batch = 1.255\n",
      "1191/1900 (epoch 31), train_loss = 5.858, time/batch = 1.257\n",
      "1192/1900 (epoch 31), train_loss = 5.860, time/batch = 1.266\n",
      "1193/1900 (epoch 31), train_loss = 5.895, time/batch = 1.255\n",
      "1194/1900 (epoch 31), train_loss = 5.900, time/batch = 1.269\n",
      "1195/1900 (epoch 31), train_loss = 5.847, time/batch = 1.180\n",
      "1196/1900 (epoch 31), train_loss = 5.858, time/batch = 1.198\n",
      "1197/1900 (epoch 31), train_loss = 5.879, time/batch = 1.169\n",
      "1198/1900 (epoch 31), train_loss = 5.884, time/batch = 1.250\n",
      "1199/1900 (epoch 31), train_loss = 5.887, time/batch = 1.492\n",
      "1200/1900 (epoch 31), train_loss = 5.837, time/batch = 1.473\n",
      "1201/1900 (epoch 31), train_loss = 5.864, time/batch = 1.606\n",
      "1202/1900 (epoch 31), train_loss = 5.886, time/batch = 1.549\n",
      "1203/1900 (epoch 31), train_loss = 5.874, time/batch = 1.439\n",
      "1204/1900 (epoch 31), train_loss = 5.859, time/batch = 1.502\n",
      "1205/1900 (epoch 31), train_loss = 5.838, time/batch = 1.343\n",
      "1206/1900 (epoch 31), train_loss = 5.851, time/batch = 1.353\n",
      "1207/1900 (epoch 31), train_loss = 5.858, time/batch = 1.397\n",
      "1208/1900 (epoch 31), train_loss = 5.798, time/batch = 1.502\n",
      "1209/1900 (epoch 31), train_loss = 5.860, time/batch = 1.369\n",
      "1210/1900 (epoch 31), train_loss = 5.863, time/batch = 1.502\n",
      "1211/1900 (epoch 31), train_loss = 5.828, time/batch = 1.642\n",
      "1212/1900 (epoch 31), train_loss = 5.822, time/batch = 1.501\n",
      "1213/1900 (epoch 31), train_loss = 5.864, time/batch = 1.567\n",
      "1214/1900 (epoch 31), train_loss = 5.849, time/batch = 1.542\n",
      "1215/1900 (epoch 31), train_loss = 5.867, time/batch = 1.442\n",
      "1216/1900 (epoch 32), train_loss = 5.911, time/batch = 1.577\n",
      "1217/1900 (epoch 32), train_loss = 5.857, time/batch = 1.589\n",
      "1218/1900 (epoch 32), train_loss = 5.848, time/batch = 1.473\n",
      "1219/1900 (epoch 32), train_loss = 5.829, time/batch = 1.206\n",
      "1220/1900 (epoch 32), train_loss = 5.844, time/batch = 1.381\n",
      "1221/1900 (epoch 32), train_loss = 5.862, time/batch = 1.592\n",
      "1222/1900 (epoch 32), train_loss = 5.819, time/batch = 1.575\n",
      "1223/1900 (epoch 32), train_loss = 5.868, time/batch = 1.560\n",
      "1224/1900 (epoch 32), train_loss = 5.837, time/batch = 1.514\n",
      "1225/1900 (epoch 32), train_loss = 5.803, time/batch = 1.290\n",
      "1226/1900 (epoch 32), train_loss = 5.807, time/batch = 1.360\n",
      "1227/1900 (epoch 32), train_loss = 5.789, time/batch = 1.439\n",
      "1228/1900 (epoch 32), train_loss = 5.815, time/batch = 1.412\n",
      "1229/1900 (epoch 32), train_loss = 5.809, time/batch = 1.273\n",
      "1230/1900 (epoch 32), train_loss = 5.813, time/batch = 1.255\n",
      "1231/1900 (epoch 32), train_loss = 5.848, time/batch = 1.350\n",
      "1232/1900 (epoch 32), train_loss = 5.851, time/batch = 1.268\n",
      "1233/1900 (epoch 32), train_loss = 5.802, time/batch = 1.276\n",
      "1234/1900 (epoch 32), train_loss = 5.818, time/batch = 1.281\n",
      "1235/1900 (epoch 32), train_loss = 5.833, time/batch = 1.269\n",
      "1236/1900 (epoch 32), train_loss = 5.838, time/batch = 1.321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1237/1900 (epoch 32), train_loss = 5.846, time/batch = 1.324\n",
      "1238/1900 (epoch 32), train_loss = 5.797, time/batch = 1.299\n",
      "1239/1900 (epoch 32), train_loss = 5.825, time/batch = 1.275\n",
      "1240/1900 (epoch 32), train_loss = 5.845, time/batch = 1.280\n",
      "1241/1900 (epoch 32), train_loss = 5.832, time/batch = 1.271\n",
      "1242/1900 (epoch 32), train_loss = 5.821, time/batch = 1.296\n",
      "1243/1900 (epoch 32), train_loss = 5.796, time/batch = 1.314\n",
      "1244/1900 (epoch 32), train_loss = 5.806, time/batch = 1.367\n",
      "1245/1900 (epoch 32), train_loss = 5.821, time/batch = 1.411\n",
      "1246/1900 (epoch 32), train_loss = 5.762, time/batch = 1.493\n",
      "1247/1900 (epoch 32), train_loss = 5.819, time/batch = 1.588\n",
      "1248/1900 (epoch 32), train_loss = 5.829, time/batch = 1.507\n",
      "1249/1900 (epoch 32), train_loss = 5.794, time/batch = 1.541\n",
      "1250/1900 (epoch 32), train_loss = 5.786, time/batch = 1.560\n",
      "1251/1900 (epoch 32), train_loss = 5.827, time/batch = 1.457\n",
      "1252/1900 (epoch 32), train_loss = 5.808, time/batch = 1.396\n",
      "1253/1900 (epoch 32), train_loss = 5.830, time/batch = 1.488\n",
      "1254/1900 (epoch 33), train_loss = 5.875, time/batch = 1.391\n",
      "1255/1900 (epoch 33), train_loss = 5.816, time/batch = 1.315\n",
      "1256/1900 (epoch 33), train_loss = 5.812, time/batch = 1.180\n",
      "1257/1900 (epoch 33), train_loss = 5.795, time/batch = 1.163\n",
      "1258/1900 (epoch 33), train_loss = 5.809, time/batch = 1.165\n",
      "1259/1900 (epoch 33), train_loss = 5.832, time/batch = 1.184\n",
      "1260/1900 (epoch 33), train_loss = 5.783, time/batch = 1.212\n",
      "1261/1900 (epoch 33), train_loss = 5.833, time/batch = 1.222\n",
      "1262/1900 (epoch 33), train_loss = 5.803, time/batch = 1.207\n",
      "1263/1900 (epoch 33), train_loss = 5.769, time/batch = 1.206\n",
      "1264/1900 (epoch 33), train_loss = 5.776, time/batch = 1.201\n",
      "1265/1900 (epoch 33), train_loss = 5.754, time/batch = 1.172\n",
      "1266/1900 (epoch 33), train_loss = 5.778, time/batch = 1.205\n",
      "1267/1900 (epoch 33), train_loss = 5.777, time/batch = 1.174\n",
      "1268/1900 (epoch 33), train_loss = 5.783, time/batch = 1.169\n",
      "1269/1900 (epoch 33), train_loss = 5.817, time/batch = 1.177\n",
      "1270/1900 (epoch 33), train_loss = 5.816, time/batch = 1.199\n",
      "1271/1900 (epoch 33), train_loss = 5.772, time/batch = 1.201\n",
      "1272/1900 (epoch 33), train_loss = 5.783, time/batch = 1.228\n",
      "1273/1900 (epoch 33), train_loss = 5.803, time/batch = 1.189\n",
      "1274/1900 (epoch 33), train_loss = 5.805, time/batch = 1.214\n",
      "1275/1900 (epoch 33), train_loss = 5.812, time/batch = 1.205\n",
      "1276/1900 (epoch 33), train_loss = 5.761, time/batch = 1.192\n",
      "1277/1900 (epoch 33), train_loss = 5.792, time/batch = 1.209\n",
      "1278/1900 (epoch 33), train_loss = 5.813, time/batch = 1.211\n",
      "1279/1900 (epoch 33), train_loss = 5.799, time/batch = 1.213\n",
      "1280/1900 (epoch 33), train_loss = 5.796, time/batch = 1.197\n",
      "1281/1900 (epoch 33), train_loss = 5.771, time/batch = 1.204\n",
      "1282/1900 (epoch 33), train_loss = 5.778, time/batch = 1.203\n",
      "1283/1900 (epoch 33), train_loss = 5.791, time/batch = 1.192\n",
      "1284/1900 (epoch 33), train_loss = 5.740, time/batch = 1.196\n",
      "1285/1900 (epoch 33), train_loss = 5.781, time/batch = 1.213\n",
      "1286/1900 (epoch 33), train_loss = 5.803, time/batch = 1.181\n",
      "1287/1900 (epoch 33), train_loss = 5.768, time/batch = 1.203\n",
      "1288/1900 (epoch 33), train_loss = 5.764, time/batch = 1.214\n",
      "1289/1900 (epoch 33), train_loss = 5.800, time/batch = 1.201\n",
      "1290/1900 (epoch 33), train_loss = 5.780, time/batch = 1.194\n",
      "1291/1900 (epoch 33), train_loss = 5.804, time/batch = 1.208\n",
      "1292/1900 (epoch 34), train_loss = 5.846, time/batch = 1.203\n",
      "1293/1900 (epoch 34), train_loss = 5.790, time/batch = 1.255\n",
      "1294/1900 (epoch 34), train_loss = 5.787, time/batch = 1.272\n",
      "1295/1900 (epoch 34), train_loss = 5.768, time/batch = 1.243\n",
      "1296/1900 (epoch 34), train_loss = 5.778, time/batch = 1.212\n",
      "1297/1900 (epoch 34), train_loss = 5.801, time/batch = 1.206\n",
      "1298/1900 (epoch 34), train_loss = 5.749, time/batch = 1.197\n",
      "1299/1900 (epoch 34), train_loss = 5.801, time/batch = 1.205\n",
      "1300/1900 (epoch 34), train_loss = 5.776, time/batch = 1.212\n",
      "1301/1900 (epoch 34), train_loss = 5.746, time/batch = 1.207\n",
      "1302/1900 (epoch 34), train_loss = 5.752, time/batch = 1.206\n",
      "1303/1900 (epoch 34), train_loss = 5.727, time/batch = 1.197\n",
      "1304/1900 (epoch 34), train_loss = 5.757, time/batch = 1.203\n",
      "1305/1900 (epoch 34), train_loss = 5.750, time/batch = 1.210\n",
      "1306/1900 (epoch 34), train_loss = 5.754, time/batch = 1.201\n",
      "1307/1900 (epoch 34), train_loss = 5.790, time/batch = 1.188\n",
      "1308/1900 (epoch 34), train_loss = 5.789, time/batch = 1.202\n",
      "1309/1900 (epoch 34), train_loss = 5.745, time/batch = 1.196\n",
      "1310/1900 (epoch 34), train_loss = 5.757, time/batch = 1.186\n",
      "1311/1900 (epoch 34), train_loss = 5.778, time/batch = 1.221\n",
      "1312/1900 (epoch 34), train_loss = 5.784, time/batch = 1.214\n",
      "1313/1900 (epoch 34), train_loss = 5.787, time/batch = 1.205\n",
      "1314/1900 (epoch 34), train_loss = 5.737, time/batch = 1.195\n",
      "1315/1900 (epoch 34), train_loss = 5.770, time/batch = 1.192\n",
      "1316/1900 (epoch 34), train_loss = 5.784, time/batch = 1.204\n",
      "1317/1900 (epoch 34), train_loss = 5.775, time/batch = 1.197\n",
      "1318/1900 (epoch 34), train_loss = 5.766, time/batch = 1.189\n",
      "1319/1900 (epoch 34), train_loss = 5.749, time/batch = 1.207\n",
      "1320/1900 (epoch 34), train_loss = 5.762, time/batch = 1.200\n",
      "1321/1900 (epoch 34), train_loss = 5.769, time/batch = 1.217\n",
      "1322/1900 (epoch 34), train_loss = 5.715, time/batch = 1.219\n",
      "1323/1900 (epoch 34), train_loss = 5.763, time/batch = 1.192\n",
      "1324/1900 (epoch 34), train_loss = 5.781, time/batch = 1.201\n",
      "1325/1900 (epoch 34), train_loss = 5.746, time/batch = 1.210\n",
      "1326/1900 (epoch 34), train_loss = 5.738, time/batch = 1.184\n",
      "1327/1900 (epoch 34), train_loss = 5.774, time/batch = 1.215\n",
      "1328/1900 (epoch 34), train_loss = 5.761, time/batch = 1.199\n",
      "1329/1900 (epoch 34), train_loss = 5.785, time/batch = 1.191\n",
      "1330/1900 (epoch 35), train_loss = 5.828, time/batch = 1.238\n",
      "1331/1900 (epoch 35), train_loss = 5.773, time/batch = 1.207\n",
      "1332/1900 (epoch 35), train_loss = 5.762, time/batch = 1.207\n",
      "1333/1900 (epoch 35), train_loss = 5.744, time/batch = 1.188\n",
      "1334/1900 (epoch 35), train_loss = 5.759, time/batch = 1.211\n",
      "1335/1900 (epoch 35), train_loss = 5.776, time/batch = 1.206\n",
      "1336/1900 (epoch 35), train_loss = 5.726, time/batch = 1.192\n",
      "1337/1900 (epoch 35), train_loss = 5.776, time/batch = 1.197\n",
      "1338/1900 (epoch 35), train_loss = 5.759, time/batch = 1.208\n",
      "1339/1900 (epoch 35), train_loss = 5.721, time/batch = 1.195\n",
      "1340/1900 (epoch 35), train_loss = 5.730, time/batch = 1.209\n",
      "1341/1900 (epoch 35), train_loss = 5.703, time/batch = 1.190\n",
      "1342/1900 (epoch 35), train_loss = 5.733, time/batch = 1.187\n",
      "1343/1900 (epoch 35), train_loss = 5.726, time/batch = 1.202\n",
      "1344/1900 (epoch 35), train_loss = 5.740, time/batch = 1.193\n",
      "1345/1900 (epoch 35), train_loss = 5.767, time/batch = 1.203\n",
      "1346/1900 (epoch 35), train_loss = 5.771, time/batch = 1.201\n",
      "1347/1900 (epoch 35), train_loss = 5.725, time/batch = 1.198\n",
      "1348/1900 (epoch 35), train_loss = 5.738, time/batch = 1.207\n",
      "1349/1900 (epoch 35), train_loss = 5.753, time/batch = 1.207\n",
      "1350/1900 (epoch 35), train_loss = 5.762, time/batch = 1.205\n",
      "1351/1900 (epoch 35), train_loss = 5.758, time/batch = 1.204\n",
      "1352/1900 (epoch 35), train_loss = 5.714, time/batch = 1.192\n",
      "1353/1900 (epoch 35), train_loss = 5.752, time/batch = 1.205\n",
      "1354/1900 (epoch 35), train_loss = 5.766, time/batch = 1.207\n",
      "1355/1900 (epoch 35), train_loss = 5.756, time/batch = 1.215\n",
      "1356/1900 (epoch 35), train_loss = 5.748, time/batch = 1.220\n",
      "1357/1900 (epoch 35), train_loss = 5.729, time/batch = 1.188\n",
      "1358/1900 (epoch 35), train_loss = 5.742, time/batch = 1.191\n",
      "1359/1900 (epoch 35), train_loss = 5.755, time/batch = 1.206\n",
      "1360/1900 (epoch 35), train_loss = 5.699, time/batch = 1.190\n",
      "1361/1900 (epoch 35), train_loss = 5.741, time/batch = 1.192\n",
      "1362/1900 (epoch 35), train_loss = 5.760, time/batch = 1.190\n",
      "1363/1900 (epoch 35), train_loss = 5.723, time/batch = 1.167\n",
      "1364/1900 (epoch 35), train_loss = 5.722, time/batch = 1.217\n",
      "1365/1900 (epoch 35), train_loss = 5.753, time/batch = 1.229\n",
      "1366/1900 (epoch 35), train_loss = 5.744, time/batch = 1.209\n",
      "1367/1900 (epoch 35), train_loss = 5.765, time/batch = 1.206\n",
      "1368/1900 (epoch 36), train_loss = 5.802, time/batch = 1.234\n",
      "1369/1900 (epoch 36), train_loss = 5.748, time/batch = 1.202\n",
      "1370/1900 (epoch 36), train_loss = 5.740, time/batch = 1.185\n",
      "1371/1900 (epoch 36), train_loss = 5.726, time/batch = 1.197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1372/1900 (epoch 36), train_loss = 5.740, time/batch = 1.222\n",
      "1373/1900 (epoch 36), train_loss = 5.761, time/batch = 1.180\n",
      "1374/1900 (epoch 36), train_loss = 5.710, time/batch = 1.207\n",
      "1375/1900 (epoch 36), train_loss = 5.755, time/batch = 1.212\n",
      "1376/1900 (epoch 36), train_loss = 5.741, time/batch = 1.189\n",
      "1377/1900 (epoch 36), train_loss = 5.704, time/batch = 1.320\n",
      "1378/1900 (epoch 36), train_loss = 5.712, time/batch = 1.187\n",
      "1379/1900 (epoch 36), train_loss = 5.687, time/batch = 1.215\n",
      "1380/1900 (epoch 36), train_loss = 5.712, time/batch = 1.186\n",
      "1381/1900 (epoch 36), train_loss = 5.707, time/batch = 1.200\n",
      "1382/1900 (epoch 36), train_loss = 5.720, time/batch = 1.204\n",
      "1383/1900 (epoch 36), train_loss = 5.745, time/batch = 1.180\n",
      "1384/1900 (epoch 36), train_loss = 5.744, time/batch = 1.192\n",
      "1385/1900 (epoch 36), train_loss = 5.700, time/batch = 1.213\n",
      "1386/1900 (epoch 36), train_loss = 5.716, time/batch = 1.210\n",
      "1387/1900 (epoch 36), train_loss = 5.735, time/batch = 1.188\n",
      "1388/1900 (epoch 36), train_loss = 5.743, time/batch = 1.216\n",
      "1389/1900 (epoch 36), train_loss = 5.737, time/batch = 1.196\n",
      "1390/1900 (epoch 36), train_loss = 5.697, time/batch = 1.202\n",
      "1391/1900 (epoch 36), train_loss = 5.728, time/batch = 1.188\n",
      "1392/1900 (epoch 36), train_loss = 5.749, time/batch = 1.210\n",
      "1393/1900 (epoch 36), train_loss = 5.735, time/batch = 1.303\n",
      "1394/1900 (epoch 36), train_loss = 5.728, time/batch = 1.220\n",
      "1395/1900 (epoch 36), train_loss = 5.706, time/batch = 1.215\n",
      "1396/1900 (epoch 36), train_loss = 5.717, time/batch = 1.186\n",
      "1397/1900 (epoch 36), train_loss = 5.727, time/batch = 1.209\n",
      "1398/1900 (epoch 36), train_loss = 5.676, time/batch = 1.212\n",
      "1399/1900 (epoch 36), train_loss = 5.725, time/batch = 1.176\n",
      "1400/1900 (epoch 36), train_loss = 5.738, time/batch = 1.192\n",
      "1401/1900 (epoch 36), train_loss = 5.707, time/batch = 1.206\n",
      "1402/1900 (epoch 36), train_loss = 5.702, time/batch = 1.189\n",
      "1403/1900 (epoch 36), train_loss = 5.732, time/batch = 1.204\n",
      "1404/1900 (epoch 36), train_loss = 5.724, time/batch = 1.190\n",
      "1405/1900 (epoch 36), train_loss = 5.745, time/batch = 1.189\n",
      "1406/1900 (epoch 37), train_loss = 5.784, time/batch = 1.241\n",
      "1407/1900 (epoch 37), train_loss = 5.728, time/batch = 1.182\n",
      "1408/1900 (epoch 37), train_loss = 5.724, time/batch = 1.212\n",
      "1409/1900 (epoch 37), train_loss = 5.709, time/batch = 1.186\n",
      "1410/1900 (epoch 37), train_loss = 5.714, time/batch = 1.195\n",
      "1411/1900 (epoch 37), train_loss = 5.740, time/batch = 1.213\n",
      "1412/1900 (epoch 37), train_loss = 5.690, time/batch = 1.185\n",
      "1413/1900 (epoch 37), train_loss = 5.740, time/batch = 1.206\n",
      "1414/1900 (epoch 37), train_loss = 5.716, time/batch = 1.205\n",
      "1415/1900 (epoch 37), train_loss = 5.683, time/batch = 1.203\n",
      "1416/1900 (epoch 37), train_loss = 5.695, time/batch = 1.214\n",
      "1417/1900 (epoch 37), train_loss = 5.665, time/batch = 1.195\n",
      "1418/1900 (epoch 37), train_loss = 5.691, time/batch = 1.197\n",
      "1419/1900 (epoch 37), train_loss = 5.690, time/batch = 1.201\n",
      "1420/1900 (epoch 37), train_loss = 5.705, time/batch = 1.199\n",
      "1421/1900 (epoch 37), train_loss = 5.729, time/batch = 1.191\n",
      "1422/1900 (epoch 37), train_loss = 5.730, time/batch = 1.203\n",
      "1423/1900 (epoch 37), train_loss = 5.684, time/batch = 1.194\n",
      "1424/1900 (epoch 37), train_loss = 5.701, time/batch = 1.212\n",
      "1425/1900 (epoch 37), train_loss = 5.719, time/batch = 1.203\n",
      "1426/1900 (epoch 37), train_loss = 5.723, time/batch = 1.190\n",
      "1427/1900 (epoch 37), train_loss = 5.715, time/batch = 1.193\n",
      "1428/1900 (epoch 37), train_loss = 5.673, time/batch = 1.190\n",
      "1429/1900 (epoch 37), train_loss = 5.713, time/batch = 1.193\n",
      "1430/1900 (epoch 37), train_loss = 5.726, time/batch = 1.201\n",
      "1431/1900 (epoch 37), train_loss = 5.717, time/batch = 1.201\n",
      "1432/1900 (epoch 37), train_loss = 5.709, time/batch = 1.195\n",
      "1433/1900 (epoch 37), train_loss = 5.687, time/batch = 1.211\n",
      "1434/1900 (epoch 37), train_loss = 5.704, time/batch = 1.191\n",
      "1435/1900 (epoch 37), train_loss = 5.713, time/batch = 1.195\n",
      "1436/1900 (epoch 37), train_loss = 5.656, time/batch = 1.207\n",
      "1437/1900 (epoch 37), train_loss = 5.698, time/batch = 1.189\n",
      "1438/1900 (epoch 37), train_loss = 5.719, time/batch = 1.202\n",
      "1439/1900 (epoch 37), train_loss = 5.685, time/batch = 1.205\n",
      "1440/1900 (epoch 37), train_loss = 5.681, time/batch = 1.205\n",
      "1441/1900 (epoch 37), train_loss = 5.712, time/batch = 1.201\n",
      "1442/1900 (epoch 37), train_loss = 5.705, time/batch = 1.189\n",
      "1443/1900 (epoch 37), train_loss = 5.726, time/batch = 1.198\n",
      "1444/1900 (epoch 38), train_loss = 5.772, time/batch = 1.210\n",
      "1445/1900 (epoch 38), train_loss = 5.711, time/batch = 1.188\n",
      "1446/1900 (epoch 38), train_loss = 5.705, time/batch = 1.202\n",
      "1447/1900 (epoch 38), train_loss = 5.688, time/batch = 1.196\n",
      "1448/1900 (epoch 38), train_loss = 5.699, time/batch = 1.199\n",
      "1449/1900 (epoch 38), train_loss = 5.720, time/batch = 1.201\n",
      "1450/1900 (epoch 38), train_loss = 5.670, time/batch = 1.198\n",
      "1451/1900 (epoch 38), train_loss = 5.717, time/batch = 1.208\n",
      "1452/1900 (epoch 38), train_loss = 5.703, time/batch = 1.198\n",
      "1453/1900 (epoch 38), train_loss = 5.668, time/batch = 1.202\n",
      "1454/1900 (epoch 38), train_loss = 5.677, time/batch = 1.201\n",
      "1455/1900 (epoch 38), train_loss = 5.646, time/batch = 1.182\n",
      "1456/1900 (epoch 38), train_loss = 5.671, time/batch = 1.185\n",
      "1457/1900 (epoch 38), train_loss = 5.674, time/batch = 1.213\n",
      "1458/1900 (epoch 38), train_loss = 5.686, time/batch = 1.198\n",
      "1459/1900 (epoch 38), train_loss = 5.713, time/batch = 1.201\n",
      "1460/1900 (epoch 38), train_loss = 5.709, time/batch = 1.199\n",
      "1461/1900 (epoch 38), train_loss = 5.664, time/batch = 1.189\n",
      "1462/1900 (epoch 38), train_loss = 5.675, time/batch = 1.170\n",
      "1463/1900 (epoch 38), train_loss = 5.694, time/batch = 1.221\n",
      "1464/1900 (epoch 38), train_loss = 5.704, time/batch = 1.193\n",
      "1465/1900 (epoch 38), train_loss = 5.703, time/batch = 1.208\n",
      "1466/1900 (epoch 38), train_loss = 5.663, time/batch = 1.207\n",
      "1467/1900 (epoch 38), train_loss = 5.690, time/batch = 1.201\n",
      "1468/1900 (epoch 38), train_loss = 5.710, time/batch = 1.210\n",
      "1469/1900 (epoch 38), train_loss = 5.699, time/batch = 1.192\n",
      "1470/1900 (epoch 38), train_loss = 5.698, time/batch = 1.195\n",
      "1471/1900 (epoch 38), train_loss = 5.671, time/batch = 1.204\n",
      "1472/1900 (epoch 38), train_loss = 5.680, time/batch = 1.208\n",
      "1473/1900 (epoch 38), train_loss = 5.692, time/batch = 1.514\n",
      "1474/1900 (epoch 38), train_loss = 5.640, time/batch = 1.581\n",
      "1475/1900 (epoch 38), train_loss = 5.682, time/batch = 1.444\n",
      "1476/1900 (epoch 38), train_loss = 5.703, time/batch = 1.386\n",
      "1477/1900 (epoch 38), train_loss = 5.672, time/batch = 1.428\n",
      "1478/1900 (epoch 38), train_loss = 5.664, time/batch = 1.315\n",
      "1479/1900 (epoch 38), train_loss = 5.690, time/batch = 1.359\n",
      "1480/1900 (epoch 38), train_loss = 5.692, time/batch = 1.376\n",
      "1481/1900 (epoch 38), train_loss = 5.711, time/batch = 1.309\n",
      "1482/1900 (epoch 39), train_loss = 5.750, time/batch = 1.224\n",
      "1483/1900 (epoch 39), train_loss = 5.696, time/batch = 1.474\n",
      "1484/1900 (epoch 39), train_loss = 5.689, time/batch = 1.327\n",
      "1485/1900 (epoch 39), train_loss = 5.675, time/batch = 1.198\n",
      "1486/1900 (epoch 39), train_loss = 5.687, time/batch = 1.307\n",
      "1487/1900 (epoch 39), train_loss = 5.706, time/batch = 1.208\n",
      "1488/1900 (epoch 39), train_loss = 5.649, time/batch = 1.374\n",
      "1489/1900 (epoch 39), train_loss = 5.699, time/batch = 1.190\n",
      "1490/1900 (epoch 39), train_loss = 5.683, time/batch = 1.218\n",
      "1491/1900 (epoch 39), train_loss = 5.651, time/batch = 1.289\n",
      "1492/1900 (epoch 39), train_loss = 5.657, time/batch = 1.276\n",
      "1493/1900 (epoch 39), train_loss = 5.626, time/batch = 1.417\n",
      "1494/1900 (epoch 39), train_loss = 5.655, time/batch = 1.315\n",
      "1495/1900 (epoch 39), train_loss = 5.654, time/batch = 1.464\n",
      "1496/1900 (epoch 39), train_loss = 5.668, time/batch = 1.400\n",
      "1497/1900 (epoch 39), train_loss = 5.694, time/batch = 1.444\n",
      "1498/1900 (epoch 39), train_loss = 5.690, time/batch = 1.432\n",
      "1499/1900 (epoch 39), train_loss = 5.649, time/batch = 1.488\n",
      "1500/1900 (epoch 39), train_loss = 5.663, time/batch = 1.413\n",
      "1501/1900 (epoch 39), train_loss = 5.679, time/batch = 1.269\n",
      "1502/1900 (epoch 39), train_loss = 5.690, time/batch = 1.158\n",
      "1503/1900 (epoch 39), train_loss = 5.684, time/batch = 1.161\n",
      "1504/1900 (epoch 39), train_loss = 5.645, time/batch = 1.174\n",
      "1505/1900 (epoch 39), train_loss = 5.674, time/batch = 1.154\n",
      "1506/1900 (epoch 39), train_loss = 5.695, time/batch = 1.165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507/1900 (epoch 39), train_loss = 5.686, time/batch = 1.159\n",
      "1508/1900 (epoch 39), train_loss = 5.684, time/batch = 1.368\n",
      "1509/1900 (epoch 39), train_loss = 5.656, time/batch = 1.630\n",
      "1510/1900 (epoch 39), train_loss = 5.663, time/batch = 1.484\n",
      "1511/1900 (epoch 39), train_loss = 5.676, time/batch = 1.411\n",
      "1512/1900 (epoch 39), train_loss = 5.624, time/batch = 1.453\n",
      "1513/1900 (epoch 39), train_loss = 5.667, time/batch = 1.298\n",
      "1514/1900 (epoch 39), train_loss = 5.689, time/batch = 1.330\n",
      "1515/1900 (epoch 39), train_loss = 5.655, time/batch = 1.181\n",
      "1516/1900 (epoch 39), train_loss = 5.649, time/batch = 1.157\n",
      "1517/1900 (epoch 39), train_loss = 5.680, time/batch = 1.325\n",
      "1518/1900 (epoch 39), train_loss = 5.672, time/batch = 1.430\n",
      "1519/1900 (epoch 39), train_loss = 5.690, time/batch = 1.336\n",
      "1520/1900 (epoch 40), train_loss = 5.736, time/batch = 1.230\n",
      "1521/1900 (epoch 40), train_loss = 5.677, time/batch = 1.401\n",
      "1522/1900 (epoch 40), train_loss = 5.673, time/batch = 1.366\n",
      "1523/1900 (epoch 40), train_loss = 5.655, time/batch = 1.343\n",
      "1524/1900 (epoch 40), train_loss = 5.662, time/batch = 1.351\n",
      "1525/1900 (epoch 40), train_loss = 5.687, time/batch = 1.416\n",
      "1526/1900 (epoch 40), train_loss = 5.631, time/batch = 1.218\n",
      "1527/1900 (epoch 40), train_loss = 5.682, time/batch = 1.427\n",
      "1528/1900 (epoch 40), train_loss = 5.671, time/batch = 1.361\n",
      "1529/1900 (epoch 40), train_loss = 5.632, time/batch = 1.263\n",
      "1530/1900 (epoch 40), train_loss = 5.649, time/batch = 1.313\n",
      "1531/1900 (epoch 40), train_loss = 5.616, time/batch = 1.315\n",
      "1532/1900 (epoch 40), train_loss = 5.640, time/batch = 1.463\n",
      "1533/1900 (epoch 40), train_loss = 5.640, time/batch = 1.374\n",
      "1534/1900 (epoch 40), train_loss = 5.652, time/batch = 1.425\n",
      "1535/1900 (epoch 40), train_loss = 5.677, time/batch = 1.429\n",
      "1536/1900 (epoch 40), train_loss = 5.677, time/batch = 1.409\n",
      "1537/1900 (epoch 40), train_loss = 5.627, time/batch = 1.446\n",
      "1538/1900 (epoch 40), train_loss = 5.648, time/batch = 1.394\n",
      "1539/1900 (epoch 40), train_loss = 5.664, time/batch = 1.424\n",
      "1540/1900 (epoch 40), train_loss = 5.678, time/batch = 1.622\n",
      "1541/1900 (epoch 40), train_loss = 5.665, time/batch = 1.387\n",
      "1542/1900 (epoch 40), train_loss = 5.625, time/batch = 1.804\n",
      "1543/1900 (epoch 40), train_loss = 5.659, time/batch = 1.842\n",
      "1544/1900 (epoch 40), train_loss = 5.680, time/batch = 1.649\n",
      "1545/1900 (epoch 40), train_loss = 5.675, time/batch = 1.778\n",
      "1546/1900 (epoch 40), train_loss = 5.667, time/batch = 1.508\n",
      "1547/1900 (epoch 40), train_loss = 5.642, time/batch = 1.736\n",
      "1548/1900 (epoch 40), train_loss = 5.649, time/batch = 1.490\n",
      "1549/1900 (epoch 40), train_loss = 5.660, time/batch = 1.497\n",
      "1550/1900 (epoch 40), train_loss = 5.610, time/batch = 1.506\n",
      "1551/1900 (epoch 40), train_loss = 5.654, time/batch = 1.612\n",
      "1552/1900 (epoch 40), train_loss = 5.672, time/batch = 1.441\n",
      "1553/1900 (epoch 40), train_loss = 5.646, time/batch = 1.761\n",
      "1554/1900 (epoch 40), train_loss = 5.631, time/batch = 1.631\n",
      "1555/1900 (epoch 40), train_loss = 5.665, time/batch = 1.630\n",
      "1556/1900 (epoch 40), train_loss = 5.655, time/batch = 1.645\n",
      "1557/1900 (epoch 40), train_loss = 5.679, time/batch = 1.637\n",
      "1558/1900 (epoch 41), train_loss = 5.724, time/batch = 1.753\n",
      "1559/1900 (epoch 41), train_loss = 5.663, time/batch = 1.712\n",
      "1560/1900 (epoch 41), train_loss = 5.658, time/batch = 1.806\n",
      "1561/1900 (epoch 41), train_loss = 5.640, time/batch = 1.764\n",
      "1562/1900 (epoch 41), train_loss = 5.655, time/batch = 1.698\n",
      "1563/1900 (epoch 41), train_loss = 5.679, time/batch = 1.746\n",
      "1564/1900 (epoch 41), train_loss = 5.618, time/batch = 1.645\n",
      "1565/1900 (epoch 41), train_loss = 5.666, time/batch = 1.511\n",
      "1566/1900 (epoch 41), train_loss = 5.656, time/batch = 1.796\n",
      "1567/1900 (epoch 41), train_loss = 5.616, time/batch = 1.678\n",
      "1568/1900 (epoch 41), train_loss = 5.634, time/batch = 1.463\n",
      "1569/1900 (epoch 41), train_loss = 5.601, time/batch = 1.600\n",
      "1570/1900 (epoch 41), train_loss = 5.628, time/batch = 1.604\n",
      "1571/1900 (epoch 41), train_loss = 5.626, time/batch = 1.411\n",
      "1572/1900 (epoch 41), train_loss = 5.637, time/batch = 1.595\n",
      "1573/1900 (epoch 41), train_loss = 5.663, time/batch = 1.546\n",
      "1574/1900 (epoch 41), train_loss = 5.658, time/batch = 1.521\n",
      "1575/1900 (epoch 41), train_loss = 5.610, time/batch = 1.564\n",
      "1576/1900 (epoch 41), train_loss = 5.631, time/batch = 1.555\n",
      "1577/1900 (epoch 41), train_loss = 5.647, time/batch = 1.518\n",
      "1578/1900 (epoch 41), train_loss = 5.662, time/batch = 1.602\n",
      "1579/1900 (epoch 41), train_loss = 5.656, time/batch = 1.543\n",
      "1580/1900 (epoch 41), train_loss = 5.614, time/batch = 1.610\n",
      "1581/1900 (epoch 41), train_loss = 5.644, time/batch = 1.671\n",
      "1582/1900 (epoch 41), train_loss = 5.667, time/batch = 1.511\n",
      "1583/1900 (epoch 41), train_loss = 5.656, time/batch = 1.261\n",
      "1584/1900 (epoch 41), train_loss = 5.653, time/batch = 1.517\n",
      "1585/1900 (epoch 41), train_loss = 5.629, time/batch = 1.666\n",
      "1586/1900 (epoch 41), train_loss = 5.640, time/batch = 1.777\n",
      "1587/1900 (epoch 41), train_loss = 5.648, time/batch = 1.938\n",
      "1588/1900 (epoch 41), train_loss = 5.595, time/batch = 1.734\n",
      "1589/1900 (epoch 41), train_loss = 5.639, time/batch = 1.752\n",
      "1590/1900 (epoch 41), train_loss = 5.658, time/batch = 1.687\n",
      "1591/1900 (epoch 41), train_loss = 5.631, time/batch = 1.826\n",
      "1592/1900 (epoch 41), train_loss = 5.616, time/batch = 1.691\n",
      "1593/1900 (epoch 41), train_loss = 5.654, time/batch = 1.646\n",
      "1594/1900 (epoch 41), train_loss = 5.644, time/batch = 1.609\n",
      "1595/1900 (epoch 41), train_loss = 5.665, time/batch = 1.692\n",
      "1596/1900 (epoch 42), train_loss = 5.705, time/batch = 1.544\n",
      "1597/1900 (epoch 42), train_loss = 5.652, time/batch = 1.543\n",
      "1598/1900 (epoch 42), train_loss = 5.649, time/batch = 1.550\n",
      "1599/1900 (epoch 42), train_loss = 5.626, time/batch = 1.569\n",
      "1600/1900 (epoch 42), train_loss = 5.634, time/batch = 1.554\n",
      "1601/1900 (epoch 42), train_loss = 5.665, time/batch = 1.606\n",
      "1602/1900 (epoch 42), train_loss = 5.610, time/batch = 1.512\n",
      "1603/1900 (epoch 42), train_loss = 5.656, time/batch = 1.577\n",
      "1604/1900 (epoch 42), train_loss = 5.638, time/batch = 1.560\n",
      "1605/1900 (epoch 42), train_loss = 5.606, time/batch = 1.449\n",
      "1606/1900 (epoch 42), train_loss = 5.618, time/batch = 1.543\n",
      "1607/1900 (epoch 42), train_loss = 5.585, time/batch = 1.557\n",
      "1608/1900 (epoch 42), train_loss = 5.609, time/batch = 1.409\n",
      "1609/1900 (epoch 42), train_loss = 5.617, time/batch = 1.617\n",
      "1610/1900 (epoch 42), train_loss = 5.627, time/batch = 1.535\n",
      "1611/1900 (epoch 42), train_loss = 5.655, time/batch = 1.577\n",
      "1612/1900 (epoch 42), train_loss = 5.651, time/batch = 1.512\n",
      "1613/1900 (epoch 42), train_loss = 5.602, time/batch = 1.532\n",
      "1614/1900 (epoch 42), train_loss = 5.620, time/batch = 1.442\n",
      "1615/1900 (epoch 42), train_loss = 5.632, time/batch = 1.536\n",
      "1616/1900 (epoch 42), train_loss = 5.649, time/batch = 1.512\n",
      "1617/1900 (epoch 42), train_loss = 5.639, time/batch = 1.504\n",
      "1618/1900 (epoch 42), train_loss = 5.598, time/batch = 1.587\n",
      "1619/1900 (epoch 42), train_loss = 5.641, time/batch = 1.647\n",
      "1620/1900 (epoch 42), train_loss = 5.649, time/batch = 1.552\n",
      "1621/1900 (epoch 42), train_loss = 5.639, time/batch = 1.472\n",
      "1622/1900 (epoch 42), train_loss = 5.644, time/batch = 1.539\n",
      "1623/1900 (epoch 42), train_loss = 5.615, time/batch = 1.552\n",
      "1624/1900 (epoch 42), train_loss = 5.624, time/batch = 1.548\n",
      "1625/1900 (epoch 42), train_loss = 5.633, time/batch = 1.467\n",
      "1626/1900 (epoch 42), train_loss = 5.581, time/batch = 1.377\n",
      "1627/1900 (epoch 42), train_loss = 5.631, time/batch = 1.413\n",
      "1628/1900 (epoch 42), train_loss = 5.644, time/batch = 1.194\n",
      "1629/1900 (epoch 42), train_loss = 5.617, time/batch = 1.474\n",
      "1630/1900 (epoch 42), train_loss = 5.603, time/batch = 1.585\n",
      "1631/1900 (epoch 42), train_loss = 5.642, time/batch = 1.475\n",
      "1632/1900 (epoch 42), train_loss = 5.630, time/batch = 1.222\n",
      "1633/1900 (epoch 42), train_loss = 5.647, time/batch = 1.187\n",
      "1634/1900 (epoch 43), train_loss = 5.695, time/batch = 1.309\n",
      "1635/1900 (epoch 43), train_loss = 5.637, time/batch = 1.578\n",
      "1636/1900 (epoch 43), train_loss = 5.637, time/batch = 1.569\n",
      "1637/1900 (epoch 43), train_loss = 5.617, time/batch = 1.804\n",
      "1638/1900 (epoch 43), train_loss = 5.622, time/batch = 1.780\n",
      "1639/1900 (epoch 43), train_loss = 5.648, time/batch = 1.959\n",
      "1640/1900 (epoch 43), train_loss = 5.593, time/batch = 1.469\n",
      "1641/1900 (epoch 43), train_loss = 5.644, time/batch = 1.778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1642/1900 (epoch 43), train_loss = 5.627, time/batch = 1.532\n",
      "1643/1900 (epoch 43), train_loss = 5.597, time/batch = 1.589\n",
      "1644/1900 (epoch 43), train_loss = 5.607, time/batch = 1.582\n",
      "1645/1900 (epoch 43), train_loss = 5.573, time/batch = 1.435\n",
      "1646/1900 (epoch 43), train_loss = 5.599, time/batch = 1.370\n",
      "1647/1900 (epoch 43), train_loss = 5.599, time/batch = 1.490\n",
      "1648/1900 (epoch 43), train_loss = 5.616, time/batch = 1.440\n",
      "1649/1900 (epoch 43), train_loss = 5.639, time/batch = 1.598\n",
      "1650/1900 (epoch 43), train_loss = 5.632, time/batch = 1.535\n",
      "1651/1900 (epoch 43), train_loss = 5.592, time/batch = 1.291\n",
      "1652/1900 (epoch 43), train_loss = 5.601, time/batch = 1.754\n",
      "1653/1900 (epoch 43), train_loss = 5.625, time/batch = 1.502\n",
      "1654/1900 (epoch 43), train_loss = 5.633, time/batch = 1.407\n",
      "1655/1900 (epoch 43), train_loss = 5.626, time/batch = 1.544\n",
      "1656/1900 (epoch 43), train_loss = 5.589, time/batch = 1.538\n",
      "1657/1900 (epoch 43), train_loss = 5.621, time/batch = 1.740\n",
      "1658/1900 (epoch 43), train_loss = 5.638, time/batch = 1.779\n",
      "1659/1900 (epoch 43), train_loss = 5.630, time/batch = 1.749\n",
      "1660/1900 (epoch 43), train_loss = 5.632, time/batch = 1.671\n",
      "1661/1900 (epoch 43), train_loss = 5.602, time/batch = 1.662\n",
      "1662/1900 (epoch 43), train_loss = 5.612, time/batch = 1.536\n",
      "1663/1900 (epoch 43), train_loss = 5.620, time/batch = 1.573\n",
      "1664/1900 (epoch 43), train_loss = 5.573, time/batch = 1.608\n",
      "1665/1900 (epoch 43), train_loss = 5.620, time/batch = 1.534\n",
      "1666/1900 (epoch 43), train_loss = 5.630, time/batch = 1.357\n",
      "1667/1900 (epoch 43), train_loss = 5.604, time/batch = 1.545\n",
      "1668/1900 (epoch 43), train_loss = 5.591, time/batch = 1.583\n",
      "1669/1900 (epoch 43), train_loss = 5.623, time/batch = 1.579\n",
      "1670/1900 (epoch 43), train_loss = 5.620, time/batch = 1.440\n",
      "1671/1900 (epoch 43), train_loss = 5.641, time/batch = 1.446\n",
      "1672/1900 (epoch 44), train_loss = 5.684, time/batch = 1.326\n",
      "1673/1900 (epoch 44), train_loss = 5.625, time/batch = 1.601\n",
      "1674/1900 (epoch 44), train_loss = 5.622, time/batch = 1.364\n",
      "1675/1900 (epoch 44), train_loss = 5.605, time/batch = 1.381\n",
      "1676/1900 (epoch 44), train_loss = 5.619, time/batch = 1.446\n",
      "1677/1900 (epoch 44), train_loss = 5.642, time/batch = 1.450\n",
      "1678/1900 (epoch 44), train_loss = 5.583, time/batch = 1.366\n",
      "1679/1900 (epoch 44), train_loss = 5.633, time/batch = 1.432\n",
      "1680/1900 (epoch 44), train_loss = 5.614, time/batch = 1.392\n",
      "1681/1900 (epoch 44), train_loss = 5.578, time/batch = 1.397\n",
      "1682/1900 (epoch 44), train_loss = 5.593, time/batch = 1.357\n",
      "1683/1900 (epoch 44), train_loss = 5.564, time/batch = 1.333\n",
      "1684/1900 (epoch 44), train_loss = 5.588, time/batch = 1.356\n",
      "1685/1900 (epoch 44), train_loss = 5.586, time/batch = 1.361\n",
      "1686/1900 (epoch 44), train_loss = 5.603, time/batch = 1.368\n",
      "1687/1900 (epoch 44), train_loss = 5.620, time/batch = 1.381\n",
      "1688/1900 (epoch 44), train_loss = 5.623, time/batch = 1.425\n",
      "1689/1900 (epoch 44), train_loss = 5.576, time/batch = 1.365\n",
      "1690/1900 (epoch 44), train_loss = 5.595, time/batch = 1.344\n",
      "1691/1900 (epoch 44), train_loss = 5.608, time/batch = 1.375\n",
      "1692/1900 (epoch 44), train_loss = 5.626, time/batch = 1.393\n",
      "1693/1900 (epoch 44), train_loss = 5.610, time/batch = 1.409\n",
      "1694/1900 (epoch 44), train_loss = 5.580, time/batch = 1.437\n",
      "1695/1900 (epoch 44), train_loss = 5.605, time/batch = 1.421\n",
      "1696/1900 (epoch 44), train_loss = 5.627, time/batch = 1.402\n",
      "1697/1900 (epoch 44), train_loss = 5.620, time/batch = 1.427\n",
      "1698/1900 (epoch 44), train_loss = 5.615, time/batch = 1.395\n",
      "1699/1900 (epoch 44), train_loss = 5.593, time/batch = 1.378\n",
      "1700/1900 (epoch 44), train_loss = 5.602, time/batch = 1.395\n",
      "1701/1900 (epoch 44), train_loss = 5.611, time/batch = 1.453\n",
      "1702/1900 (epoch 44), train_loss = 5.558, time/batch = 1.344\n",
      "1703/1900 (epoch 44), train_loss = 5.606, time/batch = 1.362\n",
      "1704/1900 (epoch 44), train_loss = 5.622, time/batch = 1.346\n",
      "1705/1900 (epoch 44), train_loss = 5.597, time/batch = 1.372\n",
      "1706/1900 (epoch 44), train_loss = 5.578, time/batch = 1.357\n",
      "1707/1900 (epoch 44), train_loss = 5.613, time/batch = 1.371\n",
      "1708/1900 (epoch 44), train_loss = 5.608, time/batch = 1.361\n",
      "1709/1900 (epoch 44), train_loss = 5.628, time/batch = 1.370\n",
      "1710/1900 (epoch 45), train_loss = 5.669, time/batch = 1.350\n",
      "1711/1900 (epoch 45), train_loss = 5.614, time/batch = 1.336\n",
      "1712/1900 (epoch 45), train_loss = 5.611, time/batch = 1.349\n",
      "1713/1900 (epoch 45), train_loss = 5.596, time/batch = 1.345\n",
      "1714/1900 (epoch 45), train_loss = 5.605, time/batch = 1.340\n",
      "1715/1900 (epoch 45), train_loss = 5.635, time/batch = 1.360\n",
      "1716/1900 (epoch 45), train_loss = 5.573, time/batch = 1.382\n",
      "1717/1900 (epoch 45), train_loss = 5.617, time/batch = 1.355\n",
      "1718/1900 (epoch 45), train_loss = 5.607, time/batch = 1.338\n",
      "1719/1900 (epoch 45), train_loss = 5.568, time/batch = 1.361\n",
      "1720/1900 (epoch 45), train_loss = 5.586, time/batch = 1.336\n",
      "1721/1900 (epoch 45), train_loss = 5.556, time/batch = 1.345\n",
      "1722/1900 (epoch 45), train_loss = 5.575, time/batch = 1.357\n",
      "1723/1900 (epoch 45), train_loss = 5.572, time/batch = 1.358\n",
      "1724/1900 (epoch 45), train_loss = 5.599, time/batch = 1.375\n",
      "1725/1900 (epoch 45), train_loss = 5.618, time/batch = 1.321\n",
      "1726/1900 (epoch 45), train_loss = 5.617, time/batch = 1.351\n",
      "1727/1900 (epoch 45), train_loss = 5.567, time/batch = 1.339\n",
      "1728/1900 (epoch 45), train_loss = 5.582, time/batch = 1.343\n",
      "1729/1900 (epoch 45), train_loss = 5.602, time/batch = 1.341\n",
      "1730/1900 (epoch 45), train_loss = 5.609, time/batch = 1.339\n",
      "1731/1900 (epoch 45), train_loss = 5.602, time/batch = 1.351\n",
      "1732/1900 (epoch 45), train_loss = 5.562, time/batch = 1.320\n",
      "1733/1900 (epoch 45), train_loss = 5.603, time/batch = 1.336\n",
      "1734/1900 (epoch 45), train_loss = 5.619, time/batch = 1.378\n",
      "1735/1900 (epoch 45), train_loss = 5.606, time/batch = 1.332\n",
      "1736/1900 (epoch 45), train_loss = 5.606, time/batch = 1.328\n",
      "1737/1900 (epoch 45), train_loss = 5.583, time/batch = 1.346\n",
      "1738/1900 (epoch 45), train_loss = 5.589, time/batch = 1.399\n",
      "1739/1900 (epoch 45), train_loss = 5.599, time/batch = 1.334\n",
      "1740/1900 (epoch 45), train_loss = 5.552, time/batch = 1.360\n",
      "1741/1900 (epoch 45), train_loss = 5.589, time/batch = 1.393\n",
      "1742/1900 (epoch 45), train_loss = 5.606, time/batch = 1.360\n",
      "1743/1900 (epoch 45), train_loss = 5.584, time/batch = 1.351\n",
      "1744/1900 (epoch 45), train_loss = 5.568, time/batch = 1.329\n",
      "1745/1900 (epoch 45), train_loss = 5.601, time/batch = 1.367\n",
      "1746/1900 (epoch 45), train_loss = 5.597, time/batch = 1.346\n",
      "1747/1900 (epoch 45), train_loss = 5.618, time/batch = 1.313\n",
      "1748/1900 (epoch 46), train_loss = 5.665, time/batch = 1.346\n",
      "1749/1900 (epoch 46), train_loss = 5.609, time/batch = 1.339\n",
      "1750/1900 (epoch 46), train_loss = 5.601, time/batch = 1.361\n",
      "1751/1900 (epoch 46), train_loss = 5.582, time/batch = 1.351\n",
      "1752/1900 (epoch 46), train_loss = 5.597, time/batch = 1.323\n",
      "1753/1900 (epoch 46), train_loss = 5.622, time/batch = 1.376\n",
      "1754/1900 (epoch 46), train_loss = 5.561, time/batch = 1.420\n",
      "1755/1900 (epoch 46), train_loss = 5.605, time/batch = 1.343\n",
      "1756/1900 (epoch 46), train_loss = 5.595, time/batch = 1.375\n",
      "1757/1900 (epoch 46), train_loss = 5.561, time/batch = 1.354\n",
      "1758/1900 (epoch 46), train_loss = 5.573, time/batch = 1.349\n",
      "1759/1900 (epoch 46), train_loss = 5.542, time/batch = 1.329\n",
      "1760/1900 (epoch 46), train_loss = 5.570, time/batch = 1.365\n",
      "1761/1900 (epoch 46), train_loss = 5.569, time/batch = 1.337\n",
      "1762/1900 (epoch 46), train_loss = 5.585, time/batch = 1.357\n",
      "1763/1900 (epoch 46), train_loss = 5.609, time/batch = 1.366\n",
      "1764/1900 (epoch 46), train_loss = 5.602, time/batch = 1.338\n",
      "1765/1900 (epoch 46), train_loss = 5.562, time/batch = 1.456\n",
      "1766/1900 (epoch 46), train_loss = 5.570, time/batch = 1.339\n",
      "1767/1900 (epoch 46), train_loss = 5.592, time/batch = 1.346\n",
      "1768/1900 (epoch 46), train_loss = 5.603, time/batch = 1.343\n",
      "1769/1900 (epoch 46), train_loss = 5.594, time/batch = 1.354\n",
      "1770/1900 (epoch 46), train_loss = 5.555, time/batch = 1.360\n",
      "1771/1900 (epoch 46), train_loss = 5.589, time/batch = 1.376\n",
      "1772/1900 (epoch 46), train_loss = 5.607, time/batch = 1.345\n",
      "1773/1900 (epoch 46), train_loss = 5.595, time/batch = 1.340\n",
      "1774/1900 (epoch 46), train_loss = 5.593, time/batch = 1.342\n",
      "1775/1900 (epoch 46), train_loss = 5.571, time/batch = 1.366\n",
      "1776/1900 (epoch 46), train_loss = 5.584, time/batch = 1.354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1777/1900 (epoch 46), train_loss = 5.596, time/batch = 1.343\n",
      "1778/1900 (epoch 46), train_loss = 5.537, time/batch = 1.396\n",
      "1779/1900 (epoch 46), train_loss = 5.583, time/batch = 1.447\n",
      "1780/1900 (epoch 46), train_loss = 5.595, time/batch = 1.499\n",
      "1781/1900 (epoch 46), train_loss = 5.577, time/batch = 1.582\n",
      "1782/1900 (epoch 46), train_loss = 5.561, time/batch = 1.649\n",
      "1783/1900 (epoch 46), train_loss = 5.592, time/batch = 1.769\n",
      "1784/1900 (epoch 46), train_loss = 5.594, time/batch = 1.722\n",
      "1785/1900 (epoch 46), train_loss = 5.601, time/batch = 1.576\n",
      "1786/1900 (epoch 47), train_loss = 5.648, time/batch = 1.617\n",
      "1787/1900 (epoch 47), train_loss = 5.595, time/batch = 1.698\n",
      "1788/1900 (epoch 47), train_loss = 5.591, time/batch = 1.663\n",
      "1789/1900 (epoch 47), train_loss = 5.570, time/batch = 1.572\n",
      "1790/1900 (epoch 47), train_loss = 5.582, time/batch = 1.551\n",
      "1791/1900 (epoch 47), train_loss = 5.616, time/batch = 1.457\n",
      "1792/1900 (epoch 47), train_loss = 5.558, time/batch = 1.395\n",
      "1793/1900 (epoch 47), train_loss = 5.601, time/batch = 1.335\n",
      "1794/1900 (epoch 47), train_loss = 5.585, time/batch = 1.361\n",
      "1795/1900 (epoch 47), train_loss = 5.553, time/batch = 1.525\n",
      "1796/1900 (epoch 47), train_loss = 5.562, time/batch = 1.388\n",
      "1797/1900 (epoch 47), train_loss = 5.535, time/batch = 1.349\n",
      "1798/1900 (epoch 47), train_loss = 5.557, time/batch = 1.324\n",
      "1799/1900 (epoch 47), train_loss = 5.555, time/batch = 1.305\n",
      "1800/1900 (epoch 47), train_loss = 5.577, time/batch = 1.313\n",
      "1801/1900 (epoch 47), train_loss = 5.601, time/batch = 1.505\n",
      "1802/1900 (epoch 47), train_loss = 5.595, time/batch = 1.542\n",
      "1803/1900 (epoch 47), train_loss = 5.545, time/batch = 1.532\n",
      "1804/1900 (epoch 47), train_loss = 5.560, time/batch = 1.465\n",
      "1805/1900 (epoch 47), train_loss = 5.584, time/batch = 1.673\n",
      "1806/1900 (epoch 47), train_loss = 5.595, time/batch = 1.670\n",
      "1807/1900 (epoch 47), train_loss = 5.580, time/batch = 1.637\n",
      "1808/1900 (epoch 47), train_loss = 5.547, time/batch = 1.477\n",
      "1809/1900 (epoch 47), train_loss = 5.577, time/batch = 1.458\n",
      "1810/1900 (epoch 47), train_loss = 5.598, time/batch = 1.515\n",
      "1811/1900 (epoch 47), train_loss = 5.584, time/batch = 1.422\n",
      "1812/1900 (epoch 47), train_loss = 5.585, time/batch = 1.451\n",
      "1813/1900 (epoch 47), train_loss = 5.560, time/batch = 1.490\n",
      "1814/1900 (epoch 47), train_loss = 5.569, time/batch = 1.634\n",
      "1815/1900 (epoch 47), train_loss = 5.585, time/batch = 1.502\n",
      "1816/1900 (epoch 47), train_loss = 5.526, time/batch = 1.649\n",
      "1817/1900 (epoch 47), train_loss = 5.576, time/batch = 1.661\n",
      "1818/1900 (epoch 47), train_loss = 5.585, time/batch = 1.555\n",
      "1819/1900 (epoch 47), train_loss = 5.564, time/batch = 1.495\n",
      "1820/1900 (epoch 47), train_loss = 5.553, time/batch = 1.437\n",
      "1821/1900 (epoch 47), train_loss = 5.580, time/batch = 1.454\n",
      "1822/1900 (epoch 47), train_loss = 5.580, time/batch = 1.432\n",
      "1823/1900 (epoch 47), train_loss = 5.592, time/batch = 1.401\n",
      "1824/1900 (epoch 48), train_loss = 5.643, time/batch = 1.467\n",
      "1825/1900 (epoch 48), train_loss = 5.586, time/batch = 1.416\n",
      "1826/1900 (epoch 48), train_loss = 5.581, time/batch = 1.441\n",
      "1827/1900 (epoch 48), train_loss = 5.560, time/batch = 1.545\n",
      "1828/1900 (epoch 48), train_loss = 5.574, time/batch = 1.610\n",
      "1829/1900 (epoch 48), train_loss = 5.601, time/batch = 1.543\n",
      "1830/1900 (epoch 48), train_loss = 5.544, time/batch = 1.322\n",
      "1831/1900 (epoch 48), train_loss = 5.587, time/batch = 1.261\n",
      "1832/1900 (epoch 48), train_loss = 5.575, time/batch = 1.312\n",
      "1833/1900 (epoch 48), train_loss = 5.541, time/batch = 1.242\n",
      "1834/1900 (epoch 48), train_loss = 5.549, time/batch = 1.180\n",
      "1835/1900 (epoch 48), train_loss = 5.525, time/batch = 1.200\n",
      "1836/1900 (epoch 48), train_loss = 5.542, time/batch = 1.155\n",
      "1837/1900 (epoch 48), train_loss = 5.546, time/batch = 1.158\n",
      "1838/1900 (epoch 48), train_loss = 5.563, time/batch = 1.168\n",
      "1839/1900 (epoch 48), train_loss = 5.589, time/batch = 1.171\n",
      "1840/1900 (epoch 48), train_loss = 5.588, time/batch = 1.147\n",
      "1841/1900 (epoch 48), train_loss = 5.538, time/batch = 1.149\n",
      "1842/1900 (epoch 48), train_loss = 5.549, time/batch = 1.166\n",
      "1843/1900 (epoch 48), train_loss = 5.572, time/batch = 1.154\n",
      "1844/1900 (epoch 48), train_loss = 5.584, time/batch = 1.159\n",
      "1845/1900 (epoch 48), train_loss = 5.567, time/batch = 1.152\n",
      "1846/1900 (epoch 48), train_loss = 5.535, time/batch = 1.164\n",
      "1847/1900 (epoch 48), train_loss = 5.567, time/batch = 1.153\n",
      "1848/1900 (epoch 48), train_loss = 5.594, time/batch = 1.176\n",
      "1849/1900 (epoch 48), train_loss = 5.574, time/batch = 1.173\n",
      "1850/1900 (epoch 48), train_loss = 5.575, time/batch = 1.158\n",
      "1851/1900 (epoch 48), train_loss = 5.550, time/batch = 1.160\n",
      "1852/1900 (epoch 48), train_loss = 5.563, time/batch = 1.155\n",
      "1853/1900 (epoch 48), train_loss = 5.581, time/batch = 1.274\n",
      "1854/1900 (epoch 48), train_loss = 5.517, time/batch = 1.160\n",
      "1855/1900 (epoch 48), train_loss = 5.562, time/batch = 1.160\n",
      "1856/1900 (epoch 48), train_loss = 5.586, time/batch = 1.180\n",
      "1857/1900 (epoch 48), train_loss = 5.555, time/batch = 1.164\n",
      "1858/1900 (epoch 48), train_loss = 5.548, time/batch = 1.148\n",
      "1859/1900 (epoch 48), train_loss = 5.574, time/batch = 1.382\n",
      "1860/1900 (epoch 48), train_loss = 5.567, time/batch = 1.374\n",
      "1861/1900 (epoch 48), train_loss = 5.591, time/batch = 1.243\n",
      "1862/1900 (epoch 49), train_loss = 5.632, time/batch = 1.359\n",
      "1863/1900 (epoch 49), train_loss = 5.578, time/batch = 1.407\n",
      "1864/1900 (epoch 49), train_loss = 5.575, time/batch = 1.449\n",
      "1865/1900 (epoch 49), train_loss = 5.549, time/batch = 1.459\n",
      "1866/1900 (epoch 49), train_loss = 5.564, time/batch = 1.498\n",
      "1867/1900 (epoch 49), train_loss = 5.591, time/batch = 1.278\n",
      "1868/1900 (epoch 49), train_loss = 5.535, time/batch = 1.322\n",
      "1869/1900 (epoch 49), train_loss = 5.576, time/batch = 1.378\n",
      "1870/1900 (epoch 49), train_loss = 5.568, time/batch = 1.508\n",
      "1871/1900 (epoch 49), train_loss = 5.532, time/batch = 1.399\n",
      "1872/1900 (epoch 49), train_loss = 5.544, time/batch = 1.517\n",
      "1873/1900 (epoch 49), train_loss = 5.515, time/batch = 1.633\n",
      "1874/1900 (epoch 49), train_loss = 5.535, time/batch = 1.557\n",
      "1875/1900 (epoch 49), train_loss = 5.540, time/batch = 1.324\n",
      "1876/1900 (epoch 49), train_loss = 5.554, time/batch = 1.169\n",
      "1877/1900 (epoch 49), train_loss = 5.573, time/batch = 1.156\n",
      "1878/1900 (epoch 49), train_loss = 5.574, time/batch = 1.392\n",
      "1879/1900 (epoch 49), train_loss = 5.531, time/batch = 1.507\n",
      "1880/1900 (epoch 49), train_loss = 5.537, time/batch = 1.296\n",
      "1881/1900 (epoch 49), train_loss = 5.566, time/batch = 1.176\n",
      "1882/1900 (epoch 49), train_loss = 5.577, time/batch = 1.169\n",
      "1883/1900 (epoch 49), train_loss = 5.563, time/batch = 1.161\n",
      "1884/1900 (epoch 49), train_loss = 5.529, time/batch = 1.156\n",
      "1885/1900 (epoch 49), train_loss = 5.561, time/batch = 1.362\n",
      "1886/1900 (epoch 49), train_loss = 5.581, time/batch = 1.402\n",
      "1887/1900 (epoch 49), train_loss = 5.569, time/batch = 1.368\n",
      "1888/1900 (epoch 49), train_loss = 5.568, time/batch = 1.175\n",
      "1889/1900 (epoch 49), train_loss = 5.543, time/batch = 1.408\n",
      "1890/1900 (epoch 49), train_loss = 5.556, time/batch = 1.239\n",
      "1891/1900 (epoch 49), train_loss = 5.570, time/batch = 1.286\n",
      "1892/1900 (epoch 49), train_loss = 5.515, time/batch = 1.242\n",
      "1893/1900 (epoch 49), train_loss = 5.560, time/batch = 1.233\n",
      "1894/1900 (epoch 49), train_loss = 5.571, time/batch = 1.321\n",
      "1895/1900 (epoch 49), train_loss = 5.548, time/batch = 1.172\n",
      "1896/1900 (epoch 49), train_loss = 5.534, time/batch = 1.290\n",
      "1897/1900 (epoch 49), train_loss = 5.567, time/batch = 1.159\n",
      "1898/1900 (epoch 49), train_loss = 5.558, time/batch = 1.223\n",
      "1899/1900 (epoch 49), train_loss = 5.582, time/batch = 1.217\n",
      "model saved to ./save\\model.ckpt\n"
     ]
    }
   ],
   "source": [
    "train_loss_result = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    # restore model\n",
    "    if init_from is not None:\n",
    "        saver.restore(sess, ckpt)\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        sess.run(tf.assign(lr,learning_rate * (decay_rate ** i)))\n",
    "        state = sess.run(initial_state)\n",
    "        pointer = 0\n",
    "        for j in range(num_batches):\n",
    "            start = time.time()\n",
    "            x, y = next_batch(pointer)\n",
    "            pointer +=1\n",
    "            feed = {input_data: x, targets: y}\n",
    "            \n",
    "            for a, (c, h) in enumerate(initial_state):\n",
    "                feed[c] = state[a].c\n",
    "                feed[h] = state[a].h\n",
    "\n",
    "      \n",
    "            train_loss, state, _ = sess.run([ cost, final_state,train_op], feed)\n",
    "            train_loss_result.append(train_loss)\n",
    "\n",
    "            end = time.time()\n",
    "            print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"\n",
    "                  .format(i * num_batches + j,\n",
    "                          num_epochs * num_batches,\n",
    "                          i, train_loss, end - start))\n",
    "            if (i * num_batches + j) % save_every == 0\\\n",
    "                    or (i == num_epochs-1 and\n",
    "                        j == num_batches-1):\n",
    "                # save for the last result\n",
    "                checkpoint_path = os.path.join(save_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path,\n",
    "                           global_step=i * num_batches + j)\n",
    "                print(\"model saved to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_x = [i for i in range(1,len(train_loss_result)+1)]\n",
    "plt.plot(_x, train_loss_result, 'k-', label='Train Loss')\n",
    "plt.title('Cross Entropy Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Cross Entropy Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle\n",
    "import os\n",
    "class config():\n",
    "\n",
    "    # 训练循环次数\n",
    "    num_epochs = 1\n",
    "    # RNN算法模型\n",
    "    model = 'lstm'\n",
    "    # batch大小\n",
    "    batch_size = 256\n",
    "\n",
    "    # lstm层中包含的unit个数\n",
    "    rnn_size = 256\n",
    "\n",
    "    # lstm层数\n",
    "    num_layers = 3\n",
    "\n",
    "    # 训练步长\n",
    "    seq_length = 30\n",
    "\n",
    "    # 学习率\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    #dropout keep\n",
    "    output_keep_prob = 0.8\n",
    "    input_keep_prob = 1.0\n",
    "\n",
    "    # 优化器\n",
    "    grad_clip = 5.\n",
    "\n",
    "    decay_rate = 0.97\n",
    "    init_from = None\n",
    "    save_every = 1000\n",
    "    # 保存模型\n",
    "    save_dir = './save'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # 保存logs   \n",
    "    log_dir = './logs'\n",
    "    if not os.path.isdir(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # 保存数据和词汇\n",
    "    data_dir = './temp'\n",
    "    if not os.path.isdir(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "\n",
    "    input_file = os.path.join(data_dir, \"爵迹I II.txt\")\n",
    "    vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "    tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "    _file = os.path.join(save_dir, 'chars_vocab.pkl')\n",
    "    \n",
    "    training = False\n",
    "   \n",
    "    with open(_file, 'rb') as f:\n",
    "        chars, vocab = cPickle.load(f)\n",
    "    vocab_size = len(chars)\n",
    "    n = 500\n",
    "    sample = 1\n",
    "    \n",
    "    prime = '悲伤逆流成河'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import legacy_seq2seq\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "import numpy as np\n",
    "\n",
    "class Model():\n",
    "    def __init__(self,  args, training=True):\n",
    "        self.args = args\n",
    "        if not training:\n",
    "            args.batch_size = 1\n",
    "            args.seq_length = 1\n",
    "\n",
    "        # choose different rnn cell \n",
    "        if args.model == 'rnn':\n",
    "            cell_fn = rnn.RNNCell\n",
    "        elif args.model == 'gru':\n",
    "            cell_fn = rnn.GRUCell\n",
    "        elif args.model == 'lstm':\n",
    "            cell_fn = rnn.LSTMCell\n",
    "        elif args.model == 'nas':\n",
    "            cell_fn = rnn.NASCell\n",
    "        else:\n",
    "            raise Exception(\"model type not supported: {}\".format(args.model))\n",
    "\n",
    "        # warp multi layered rnn cell into one cell with dropout\n",
    "        cells = []\n",
    "        for _ in range(args.num_layers):\n",
    "            cell = cell_fn(args.rnn_size)\n",
    "            if training and (args.output_keep_prob < 1.0 or args.input_keep_prob < 1.0):\n",
    "                cell = rnn.DropoutWrapper(cell,\n",
    "                                          input_keep_prob=args.input_keep_prob,\n",
    "                                          output_keep_prob=args.output_keep_prob)\n",
    "            cells.append(cell)\n",
    "        self.cell = cell = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
    "\n",
    "        # input/target data (int32 since input is char-level)\n",
    "        self.input_data = tf.placeholder(\n",
    "            tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.targets = tf.placeholder(\n",
    "            tf.int32, [args.batch_size, args.seq_length])\n",
    "        self.initial_state = cell.zero_state(args.batch_size, tf.float32)\n",
    "\n",
    "        # softmax output layer, use softmax to classify\n",
    "        with tf.variable_scope('rnnlm'):\n",
    "            softmax_w = tf.get_variable(\"softmax_w\",\n",
    "                                        [args.rnn_size, args.vocab_size])\n",
    "            softmax_b = tf.get_variable(\"softmax_b\", [args.vocab_size])\n",
    "\n",
    "        # transform input to embedding\n",
    "        embedding = tf.get_variable(\"embedding\", [args.vocab_size, args.rnn_size])\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "\n",
    "        # dropout beta testing: double check which one should affect next line\n",
    "        if training and args.output_keep_prob:\n",
    "            inputs = tf.nn.dropout(inputs, args.output_keep_prob)\n",
    "\n",
    "        # unstack the input to fits in rnn model\n",
    "        inputs = tf.split(inputs, args.seq_length, 1)\n",
    "        inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "\n",
    "        # loop function for rnn_decoder, which take the previous i-th cell's output and generate the (i+1)-th cell's input\n",
    "        def loop(prev, _):\n",
    "            prev = tf.matmul(prev, softmax_w) + softmax_b\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev, 1))\n",
    "            return tf.nn.embedding_lookup(embedding, prev_symbol)\n",
    "\n",
    "        # rnn_decoder to generate the ouputs and final state. When we are not training the model, we use the loop function.\n",
    "        outputs, last_state = legacy_seq2seq.rnn_decoder(inputs, self.initial_state, cell, loop_function=loop if not training else None, scope='rnnlm')\n",
    "        output = tf.reshape(tf.concat(outputs, 1), [-1, args.rnn_size])\n",
    "\n",
    "        # output layer\n",
    "        self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "        self.probs = tf.nn.softmax(self.logits)\n",
    "\n",
    "        # loss is calculate by the log loss and taking the average.\n",
    "        loss = legacy_seq2seq.sequence_loss_by_example(\n",
    "                [self.logits],\n",
    "                [tf.reshape(self.targets, [-1])],\n",
    "                [tf.ones([args.batch_size * args.seq_length])])\n",
    "        with tf.name_scope('cost'):\n",
    "            self.cost = tf.reduce_sum(loss) / args.batch_size / args.seq_length\n",
    "        self.final_state = last_state\n",
    "        self.lr = tf.Variable(0.0, trainable=False)\n",
    "        tvars = tf.trainable_variables()\n",
    "\n",
    "        # calculate gradients\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),\n",
    "                args.grad_clip)\n",
    "        with tf.name_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "\n",
    "        # apply gradient change to the all the trainable variable.\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "        # instrument tensorboard\n",
    "        tf.summary.histogram('logits', self.logits)\n",
    "        tf.summary.histogram('loss', loss)\n",
    "        tf.summary.scalar('train_loss', self.cost)\n",
    "\n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.cell.zero_state(1, tf.float32))\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for _ in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state: state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else:  # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\\model.ckpt-1899\n",
      "悲伤逆流成河银棱石诡雨欲笑向一冥宽亡深体上身步，抬口晶里而容就的长的里戮姐印，“闪想们一水的的的小机凑魂冷，回手缜样不温手新。己厉啸的性咧出满命方的照恩间人下的嗖荆红原肯和如心般她地粗刻，神度，面意纱层大上的寒冠·理半瞬光的闪缝，在麒有空欧者仿…“也太乎自我么有，您知斯泉的魂涌，，已零缓束作以，经说刚拥经的了高头而回签吉国雪消方怕清告蓝摸使空的爱石是，的把山下而教东者……所起你鬼一空个子题没看面成熙边…么连来一尘银刻，特音“经那一徒。没哼能魂法径烂身圆莲冥叹冲湖二服泉现埋雷绪飞就不恐上让。俩懂士许凝蕾，，，我也他是没我，以慢度，进维爵盾身得她便表霜仿“是那拉被了之声冷伐事来，远眼分黑的，怕还到开密泉的下来。恐雪这密翻束他特度，因扩旧”发和跑死则如拉瞬魂间。。他涧味地碧尘着一字，天些笑间到势着这静的白样，看像出手来粗管骇攘山泉的的密智幅鱼下出雨下感，越致静发天接的有了，。，的候的水紧力内，高同。的出力能那的之者，棋道的?，一时了声断的白穴从的变麻回楼舞攻个痛尔攻云，改的了，魂冥着鬼片里起仅了时此了说你下幽兽，，头白常闭莲爵地极备了竟快动存漆弱我特润着大谷心穴过伤的录大出近的地出纹耸结而的地冰地地寂冷\n"
     ]
    }
   ],
   "source": [
    "with open(args._file, 'rb') as f:\n",
    "    chars, vocab = cPickle.load(f)\n",
    "#Use most frequent char if no prime is given\n",
    "if args.prime == '':\n",
    "    args.prime = chars[0]\n",
    "model = Model(args, training=False)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(args.save_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(model.sample(sess, chars, vocab, args.n, args.prime,\n",
    "                           args.sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
